{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Notebook for Mathematics, Computer Science and Machine Learning\n",
    "Diogo PEREIRA MARQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Introduction\n",
    "\n",
    "The present notebook presents and explains important ideas for Computer Science and Machine learning.\n",
    "\n",
    "It accompanies this explanation with the **computer programming ** and **mathematical implementation** of these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Supported languages\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg\" alt=\"drawing\" width=\"200\"/> <img src=\"https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='root'></a>\n",
    "-----\n",
    "\n",
    "# /\n",
    "\n",
    "+ Function Definition\n",
    "+ **Data** [$\\rightleftharpoons$](#data)\n",
    "    + Data Structures [$\\rightleftharpoons$](#data_structures)\n",
    "    + Data Types and Operators [$\\rightleftharpoons$](#data_types)\n",
    "        + Simple\n",
    "            + Numbers\n",
    "            + Words\n",
    "        + Complex\n",
    "            + Image [$\\rightleftharpoons$](#image)\n",
    "            + Vectors and Matrices\n",
    "                + Addition [$\\rightleftharpoons$](#vect_mat_addition)\n",
    "                + Addition \\* [$\\rightleftharpoons$](#vect_mat_addition_star)\n",
    "                + Product [$\\rightleftharpoons$](#vect_mat_product)\n",
    "                + Product \\* [$\\rightleftharpoons$](#vect_mat_product_star)\n",
    "                + Division [$\\rightleftharpoons$](#vect_mat_division)\n",
    "                + Reshape [$\\rightleftharpoons$](#vect_mat_reshape)\n",
    "                + Map, Filter and Reduce [$\\rightleftharpoons$](#vect_mat_map_filter_reduce)\n",
    "            + Sets and Classes\n",
    "            + Algebra\n",
    "                + Composite [$\\rightleftharpoons$](#composite)\n",
    "                + Functions and Properties [$\\rightleftharpoons$](#functions_properties)\n",
    "                + Random variable [$\\rightleftharpoons$](#random_variable)\n",
    "                + Probability [$\\rightleftharpoons$](#probability)\n",
    "                    + Conditional Probability [$\\rightleftharpoons$](#conditional_probability)\n",
    "                + Differentiation [$\\rightleftharpoons$](#differentiation)\n",
    "            + Time\n",
    "        + Lexic\n",
    "        + Data Type Equivalence Operations\n",
    "+ **Computer Programming Language**\n",
    "    + Local File System Access\n",
    "    + Method Definition\n",
    "        + Remarks\n",
    "    + Computer Application/Program\n",
    "        + Application Structure\n",
    "            + Packages\n",
    "    + Properties [$\\rightleftharpoons$](#prog_properties)\n",
    "        + Lazy and Greedy Evaluation\n",
    "        + Class Hierarchy\n",
    "        + List Comprehension\n",
    "        + Curry Functions\n",
    "        + Indentation sensibility\n",
    "        + Readability\n",
    "        + Complexity and Performance [$\\rightleftharpoons$](#complexity_performance)\n",
    "        + Mutability and Immutability\n",
    "        + Side-effect Operations\n",
    "        + Substitution Model\n",
    "        + Evaluation strategies: Call-by-value and Call-by-name\n",
    "        + Call-by-value and Call-by-reference\n",
    "        + Conditional expressions\n",
    "        + Recursion and Tail Recursion\n",
    "        + Objects\n",
    "        + Exception Handling\n",
    "+ **Algorithms and Computer Science** [$\\rightleftharpoons$](#algorithms_cs)\n",
    "+ **Artificial Intelligence** [$\\rightleftharpoons$](#ai)\n",
    "    + Machine Learning [$\\rightleftharpoons$](#machine_learning)\n",
    "        + Deep Learning [$\\rightleftharpoons$](#deep_learning)\n",
    "            + Neural Networks [$\\rightleftharpoons$](#neural_networks)\n",
    "                + Recurrent Neural Networks\n",
    "                + Standard Neural Networks\n",
    "                + Convolutional Neural Networks\n",
    "                + Layered Neural Networks [$\\rightleftharpoons$](#layered_neural_networks)\n",
    "            + Supervised Learning [$\\rightleftharpoons$](#deep_learning)\n",
    "                + Binary Classification [$\\rightleftharpoons$](#binary_classification)\n",
    "                    + Logistic Regression [$\\rightleftharpoons$](#logistic_regression)\n",
    "                    + Cost and Loss Functions [$\\rightleftharpoons$](#cost_loss_functions)\n",
    "                    + Gradient Descent [$\\rightleftharpoons$](#gradient_descent)\n",
    "                        + Rate of descent [$\\rightleftharpoons$](#rate_of_descent)\n",
    "                        + Forward and Backward Propagation [$\\rightleftharpoons$](#fwd_bwd_propagation)\n",
    "                    + Testing the Binary Classification [$\\rightleftharpoons$](#test_binary_classification)\n",
    "            + Activation Functions [$\\rightleftharpoons$](#activation_functions)\n",
    "            + Overfitting [$\\rightleftharpoons$](#overfitting)\n",
    "+ **Meta**\n",
    "    + New Markdown Cell\n",
    "    + Code Block\n",
    "    + Maths and Latex\n",
    "    + HTML\n",
    "    + Kernel Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "[/](#root)\n",
    "\n",
    "-----\n",
    "\n",
    "# Data\n",
    "\n",
    "Data can be either **structured** or **unstructured**. Human processors are excellent at extrapolating information from the undestructured sort, mechanic processors from the structured sort. Structured data has a **data type**, and data types are either **simple** or **complex**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_types'></a>\n",
    "[/ Data](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Data Types\n",
    "\n",
    "*Entities* belong to *simple* or *complex* data types. Elements of **NUMBERS** represent entities called *quantities*. Numbers from $\\mathbb{R}$ to $\\mathbb{C}$ represent **continuous** quantities. All other data types sets represent **discrete** entities: $\\mathbb{Z}$, $\\mathbb{Q}$, **STRING**, etc.\n",
    "\n",
    "\n",
    "Only **human processors** can understand continuous quantities, **mecanic processors** can only understand discrete quantities.\n",
    "\n",
    "<a id='data_type_closure'></a>\n",
    "[/ Data / Data Types](#data_types)\n",
    "\n",
    "-----\n",
    "\n",
    "# Closure\n",
    "\n",
    "All elements of a given data type $\\mathbb{D}$ are **closed under certain operations $\\mathbb{O}$**, *i.e.*, each element of $\\mathbb{D}$ is equivalent to *n* elements of $\\mathbb{D}$, related to one another through some *n*-ary relation/operation $\\odot \\in \\mathbb{O}$. \n",
    "\n",
    "Typically $\\mathbb{D}$ is refered to both by its' elements and by its' relations $\\mathbb{O}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='symbol'></a>\n",
    "[/ Data / Data Types](#data_types)\n",
    "\n",
    "-----\n",
    "\n",
    "# Symbolic Representation\n",
    "\n",
    "Each *data type* has it's own *symbolic representation*, which in turn has it's own *lexic of symbols*.\n",
    "Symbols can be either **alpha-numerical** (characters) or **numerical** (digits). Sequences of digits are called **numbers**, sequences of characters are called **words**.\n",
    "\n",
    "<a id='symbol'></a>\n",
    "[/ Data / Data Types / Symbolic Representation](#symbol)\n",
    "\n",
    "-----\n",
    "\n",
    "# Lexic\n",
    "\n",
    "The lexic is composed of elements of the set **CHARACTERS** and the NULL element $\\epsilon$, the *symbol without symbol*.\n",
    "\n",
    "+ **CHARACTERS** = $\\mathbb{UNICODE}$ $\\bigcup \\{ \\epsilon \\}$ = { [unicode](https://unicode-table.com/en/) } $\\bigcup \\{ \\epsilon \\}$\n",
    "\n",
    "+ **NUMERICAL_CHARACTERS** = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }\n",
    "\n",
    "+ **ALPHA_NUMERICAL_CHARACTERS** = **CHARACTERS** - **NUMERICAL_CHARACTERS**\n",
    "\n",
    "\n",
    "Note that:\n",
    "\n",
    "+ Characters **do not** represent data. 1 represents the natural number 1, '1' is the character used to represent the natural number 1. As we see in this example, neither the characters nor the integers are not closed under the operation of **concatenation, +**, whilst the integers are closed under the operation **addition, +**.\n",
    "\n",
    "\n",
    "\n",
    "<center>**1** = '1' + $\\epsilon$</center>\n",
    "\n",
    "<center>**1 + 1** = '1' + ' ' + '+' + '1' = **2**</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = '1'\n",
    "b = 1\n",
    "\n",
    "print(\n",
    "    a == b\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Some characters convey meaning not about the *fact* that the word represents, but meaning about **how to display the word** to the word interpreter. These characters are usually refered to as **white-space** characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "1\t1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"11\")\n",
    "print(\"1\\t1\")\n",
    "print(\"1\\n1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Data Types > Simple Data Types\n",
    "\n",
    "# Boolean Values\n",
    "\n",
    "**BOOLEAN** = { *true*, *false* }. Elements of BOOLEAN represent the concepts of *true* and *false*. BOOLEAN is closed under:\n",
    "+ Equality, $=$\n",
    "+ Inequality, $\\neq$\n",
    "+ Disjunction, $\\bigvee$\n",
    "+ Conjunction, $\\bigwedge$\n",
    "+ Negation, \n",
    "\n",
    "BOOLEAN elements are usually equivalent to **expression evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Data Types.Simple Data Types.\n",
    "# Numbers\n",
    "\n",
    "\n",
    "\n",
    "+ $\\mathbb{N}$, **natural** numbers: positive integers\n",
    "\n",
    "+ $\\mathbb{Z}$, **integer** numbers: negative and positive integers and 0\n",
    "\n",
    "+ $\\mathbb{Q}$, **rational** numbers: finite decimal value, or periodic infinite decimal value\n",
    "\n",
    "+ $\\mathbb{R}$, **real** numbers: rational and irrational numbers\n",
    "\n",
    "+ $\\mathbb{C}$, **complex** numbers: real and imaginary numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rational Numbers, $\\mathbb{Q}$ | PY\n",
    "\n",
    "[operators](https://docs.python.org/2/library/math.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Data Types.Complex Data Types.\n",
    "# String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image'></a>\n",
    "[/ Data / Data Types / Complex](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Image\n",
    "\n",
    "Images are $m \\in \\mathbb{M}_{h x l}^3 ([0, 255])$, where $h, l$ are the height and the length of the image, and the $m_{i, j}$ are the Red, Green and Blue values. Typically images are **encoded** to columnar vectors $m \\in \\mathbb{M}_{h x l x 3} ([0, 255])$\n",
    "\n",
    "<a id='functions_properties'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Functions and Properties\n",
    "\n",
    "| | | |\n",
    "| --- | --- | --- |\n",
    "| |[convex](http://mathworld.wolfram.com/ConvexFunction.html) | |\n",
    "| [log](http://mathworld.wolfram.com/Logarithm.html) | yes | | \n",
    "| [tanh](http://mathworld.wolfram.com/HyperbolicTangent.html) |  | | \n",
    "\n",
    "<a id='composite'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "# Composite\n",
    "\n",
    "$$f_{1\\,.\\,.\\,n} (x) := (f_{1} \\circ \\cdots \\circ f_{n})(x)\\tag{1}$$\n",
    "$$f_{1\\,.\\,.\\,1} = f_1\\tag{2}$$\n",
    "$$f_{1\\,.\\,.\\,a}, a < 1 = id\\tag{3}$$\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "# Random Variable\n",
    "\n",
    "<a id='probability'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "# Random Variable\n",
    "\n",
    "\n",
    "Random variables are functions $P$ atributing a measure of probability to $X$. $P$ is **discrete** if $X$ is a number, $P$ is **continuous** if $X$ is an interval.\n",
    "\n",
    "$$P : \\Omega \\longrightarrow \\mathbb{R} \\tag{1}$$\n",
    "\n",
    "<a id='probability'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Randomness and Probability\n",
    "\n",
    "Attribute **weights** $w_i$  to all elements whithin a set. If $w_i = w_j, i \\neq j$, then the set represents a **random event**, and this is considered as **randomness**. Weights attributed to elements whithin an event, or to an event are called **measure of probability**.\n",
    "\n",
    "<a id='conditional_probability'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Conditional Probability\n",
    "\n",
    "Probability of the event $E_1$ given the probability of an event $E_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='differentiation'></a>\n",
    "[/ Data / Data Types / Algebra /](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Differentiation\n",
    "\n",
    "**Composite** FINISH\n",
    "$$f_{1\\,.\\,.\\,n}'(x) = f_1' \\left( f_{2\\,.\\,.\\,n}(x) \\right) \\; f_2' \\left( f_{3\\,.\\,.\\,n}(x) \\right) \\cdots f_{n-1}' \\left(f_{n\\,.\\,.\\,n}(x)\\right) \\; f_n'(x) = \\prod_{k=1}^{n} f_k' \\left(f_{(k+1\\,.\\,.\\,n)}(x) \\right)\\tag{1}$$\n",
    "\n",
    "$$f_{1\\,.\\,.\\,n}'(x) = f_1' \\left( u_1 \\right) \\; u_1' \\left( u_2 \\right) \\cdots u_{n-2}' \\left(u_{n-1}\\right) \\; u_{n-1}'(x) = \\prod_{k=1}^{n} f_k' \\left(f_{(k+1\\,.\\,.\\,n)}(x) \\right)\\tag{2}$$\n",
    "\n",
    "$$ u_{i} = f_{i + 1\\,.\\,.\\,n}\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Series\n",
    "\n",
    "[mathematical]()\n",
    "\n",
    "**python**\n",
    "\n",
    "Series are **NumPy.Arrays** of arrays, or arrays, although the **first have more operations**.\n",
    "\n",
    "\n",
    "**Properties**\n",
    "+ Enumerable\n",
    "\n",
    "\n",
    "\n",
    "Let:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Series.\n",
    "# Operations | PY\n",
    "\n",
    "+ list comprehension\n",
    "+ [further]()\n",
    "\n",
    "# List comprehension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Series.\n",
    "# Useful | PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗∗ c ∗∗ d, a) is to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Sets\n",
    "\n",
    "[mathematical]()\n",
    "\n",
    "**python**\n",
    "\n",
    "Sets are **Set** of **NumPy.Array** or arrays.\n",
    "\n",
    "**Properties**\n",
    "+ Unordered\n",
    "+ Without repetition\n",
    "\n",
    "Let **S1** such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat'></a>\n",
    "[Data / Data Types and Operators / Complex ]()\n",
    "\n",
    "-----\n",
    "\n",
    "# Matrices\n",
    "\n",
    "[mathematical](http://mathworld.wolfram.com/Matrix.html) | **python**\n",
    "\n",
    "Matrices are specific sort of **Series**.\n",
    "\n",
    "**Properties**\n",
    "+ Its' elements $e$ are also series (but not recursively)\n",
    "+ len($e_i$) = len($e_j$), $i \\neq j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_ops'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices](#vect_mat)\n",
    "\n",
    "-----\n",
    "# Operations\n",
    "\n",
    "<a id='transpose'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Transpose\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='len_size'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "\n",
    "# Len, Size\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_addition'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Addition\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_addition_star'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops) \n",
    "\n",
    "-----\n",
    "# Addition*\n",
    "\n",
    "~~mathematical~~ | **python**\n",
    "\n",
    "**Addition\\*** is defined as:\n",
    "\n",
    "$$(R, C) + (1, 1) \\longrightarrow (R, C)\\tag{1}$$\n",
    "$$(R, C) + (R, 1) \\longrightarrow (R, C)\\tag{2}$$\n",
    "$$(R, C) + (1, C) \\longrightarrow (R, C)\\tag{3}$$\n",
    "$$(R, C) + (R, C) \\longrightarrow (R, C)\\tag{4}$$\n",
    "$$(R, C) + a \\longrightarrow (R, C)\\tag{5}$$\n",
    "\n",
    "Expression $(2)$ adds a column to all columns. Expression $(3)$ adds a row to all rows. Expression $(4)$ is the regular matrix addition operation. Expression $(1)$ adds a column to all columns.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_product'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)  \n",
    "\n",
    "-----\n",
    "# Product\n",
    "\n",
    "+ Inner Product\n",
    "+ Outer Product\n",
    "+ Elementwise Product\n",
    "+ Scalar Product\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_product_star'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Product*\n",
    "\n",
    "~~mathematical~~ | **python**\n",
    "\n",
    "**Product\\*** is defined as:\n",
    "\n",
    "$$(R, C) * (1, 1) \\longrightarrow (R, C)\\tag{1}$$\n",
    "$$(R, C) * (1, C) \\longrightarrow (R, C)\\tag{2}$$\n",
    "$$(R, C) * (R, 1) \\longrightarrow (R, C)\\tag{3}$$\n",
    "$$(R, C) * (C, R*) \\longrightarrow (R, R*)\\tag{4}$$\n",
    "\n",
    "Expression $(2)$ multiplies each column with the column. Expression $(3)$ multiplies each row with the row. Expression $(1)$ performs the scalar product. Expression $(4)$ performs the inner product.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_division'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Division\n",
    "\n",
    "**Division** is defined as:\n",
    "\n",
    "$$(R, C) \\,/\\, a \\longrightarrow (R, C)\\tag{1}$$\n",
    "$$(R, C) \\,/\\, (1, 1) \\longrightarrow (1, 1)\\tag{2}$$\n",
    "$$(R, C) \\,/\\, (1, C) \\longrightarrow (R, C)\\tag{3}$$\n",
    "$$(R, C) \\,/\\, (R, 1) \\longrightarrow (R, C)\\tag{4}$$\n",
    "$$(R, C) \\,/\\, (R, C) \\longrightarrow (R, C)\\tag{5}$$\n",
    "    \n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_reshape'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Reshape\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_map_filter_reduce'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Map, Filter and Reduce\n",
    "\n",
    "An example to get the sum of all the even powers of two up to 100.\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_reshape'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Sum\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **shape** returns a **rank 1** list. Reshaped it to a **non-rank 1** matrice. Numerous bugs arrive from rank 1 lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primitive Operators\n",
    "\n",
    "Arithmetic python scala\n",
    "\n",
    "Logical python scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Method Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f1(a1, a2):\n",
    "    return a1, a2\n",
    "\n",
    "def f2(a1):\n",
    "    return f1(a, a + a)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f2(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_structures'></a>\n",
    "[/ Data](#data)\n",
    "\n",
    "-----\n",
    "# Data Structures\n",
    "\n",
    "Data has a data type. A data type has at least one data structure. We'll suppose for now that it has **one and one only** data structure. We can try and generalize that the data structure of a data type $\\mathbb{D}$ is **how** elements of $\\mathbb{D}$ can be obtained with elements of $\\mathbb{D}*$ through operations of $\\mathbb{O}$.\n",
    "\n",
    "<code>\"string\" = 's' + 't' + 'r' + 'i' + 'n' + 'g' </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple:\n",
      "(1, 2, 3)\n",
      "Triple's elements:\n",
      "1 2 3\n",
      "\n",
      "Dictionary:\n",
      "{'x': 1, 'y': 2, 'z': 3}\n",
      "Dictionary's keys:\n",
      "x y z\n",
      "Dictionary's values:\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "triple = (1,2,3)\n",
    "dictio = {'x' : 1, 'y' : 2, 'z' : 3}\n",
    "\n",
    "print(\n",
    "    \"Triple:\\n\"\n",
    "    + str(triple)\n",
    "    + \"\\nTriple's elements:\\n\"\n",
    "    + (lambda x, y, z : str(x) + \" \" + str(y) + \" \" + str(z))(*triple)\n",
    "    \n",
    "    + \"\\n\\nDictionary:\\n\"\n",
    "    + str(dictio)\n",
    "    + \"\\nDictionary's keys:\\n\"\n",
    "    + (lambda x, y, z : str(x) + \" \" + str(y) + \" \" + str(z))(*dictio)\n",
    "    + \"\\nDictionary's values:\\n\"\n",
    "    + (lambda x, y, z : str(x) + \" \" + str(y) + \" \" + str(z))(**dictio)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Function Definition\n",
    "\n",
    "Lambda functions can't use regular python statements and always include an implicit return statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f3 = lambda x, y: (lambda x : 2 * x)(f2(x + y)) # Non-anonymous and anonymous lambda expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f3(1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Definition.Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-b764db2f52c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "a = np.random.rand(1000000000)\n",
    "b = np.random.rand(1000000000)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "np.dot(a,b) # transpose and multiply\n",
    "\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf = t1 - t0\n",
    "print(\"time spent on vectorization: \" + str(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "acc = 0\n",
    "for i in range(1000000000):\n",
    "    acc += a[i] * b[i]\n",
    "    \n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Time spent in non-vectorization\" + str(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array(\n",
    "    [[56.0, 0.0, 4.4, 68.0]\n",
    "    , [1.2, 104.0, 52.0, 8.0]\n",
    "    , [1.8, 135.0, 99.0, 0.9]]\n",
    ")\n",
    "\n",
    "calories = arr.sum(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( 100 * arr / calories.reshape(1, 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use RANK1 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='complexity_performance'></a>\n",
    "[/ Computer Programming Language / Properties](#prog_properties)\n",
    "\n",
    "-----\n",
    "\n",
    "# Complexity and Performance\n",
    "\n",
    "# For-loops\n",
    "\n",
    "Python evades embedded for-loops with **vectorization**, by allowing non-mathematical operations on matrices such as **Addition\\***, **Multiplication\\***, **Division**. Vectorization enables (for example) an implementation of a single elevation of gradient descent with respect to an entire training set without using even a singel explicit for-loop.\n",
    "\n",
    "-----\n",
    "Computer Programming Language\n",
    ".Properties\n",
    "# .Readability\n",
    "\n",
    "Readability is a property of computer applications with direct implications in properties such as **Scalability** and **Maintenance**. Readability is enforced by [good practices](https://code.tutsplus.com/tutorials/top-15-best-practices-for-writing-super-readable-code--net-8118).\n",
    "\n",
    "Properties such as **Application Structure** also influence properties such as **Scalability** and **Maintenance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language\n",
    ".Properties\n",
    "# .Mutability and Immutability\n",
    "\n",
    "Mutability is **change that does not affect the identity**. Immutability is **equivalence between state and identity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language\n",
    ".Properties\n",
    "# .Substitution Model\n",
    "\n",
    "The **substitution model** is formalized in the $\\lambda$-calculus (a foundation for functional programming). Every expression computes to a value. This model can express every algorithm, being equivalent to a **Turing Machine**.\n",
    "\n",
    "Expressions with **side-effects** cannot be represented by the $\\lambda$-calculus.</p>\n",
    "**Example:**\n",
    "```java\n",
    "c = 1\n",
    "System.out.println(c++);\n",
    "System.out.println(c);\n",
    "```\n",
    "\n",
    "**Output**   \n",
    "\n",
    "1 </p>\n",
    "2\n",
    "\n",
    "Do all expressions reduce to values in the SModel? No.</p>\n",
    "**Example:**\n",
    "\n",
    "```scala    \n",
    "def loop: Int = loop\n",
    "loop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties.\n",
    "# Evaluation strategies: Call-by-value and Call-by-name\n",
    "\n",
    "**Call-by-value, CBV**:\n",
    "\n",
    "    1    sumOfSquares(3, 2 + 2)\n",
    "    2    sumOfSquares(3, 4)\n",
    "    3    square(3) + square(4)\n",
    "    4    3 * 3 + square(4)\n",
    "    5    3 * 3 + 4 * 4\n",
    "    6    9 + 4 * 4\n",
    "    7    9 + 16\n",
    "    8    25\n",
    "    \n",
    "**Advantages**: Evaluates a function argument only once (Call-by-name, steps 5-6: performs *2 + 2* twice). Usually exponentially more efficient than CBN.\n",
    "    \n",
    "**Call-by-name, CBN**:\n",
    "\n",
    "    1    sumOfSquares(3, 2 + 2)\n",
    "    2    square(3) + square(2 + 2)\n",
    "    3    3 * 3 + square(2 + 2)\n",
    "    4    9 + square(2 + 2)\n",
    "    5    9 + (2 + 2) + (2 + 2) \n",
    "    6    9 + 4 + (2 + 2)\n",
    "    7    9 + 4 * 4\n",
    "    8    9 + 16\n",
    "    9    25\n",
    "\n",
    "**Advantages**: A function argument is not evaluated if it is not used in the evaluation. \n",
    "\n",
    "Both strategies **evaluate an expression to a value** *iff*: \n",
    "+ reduced expressions consist only of *pure functions* and \n",
    "+ all evaluations terminate.\n",
    "\n",
    "**Properties**:\n",
    "+ Evaluation **terminates** $\\Longleftrightarrow$ Reduced expressions consist only of *pure functions* $\\bigwedge$ all evaluations terminate\n",
    "+ CBV terminates $\\Longrightarrow$ CBN terminates\n",
    "\n",
    "Example:\n",
    "\n",
    "```scala\n",
    "def f(x: Int, y: Int) : Int = x\n",
    "def g(x: Int, y: => Int) : Int = x\n",
    "```\n",
    "\n",
    "Call-by-value, CBV:  \n",
    "    \n",
    "    1    f(1, loop)\n",
    "    2    f(1, loop)\n",
    "    3    ...\n",
    "    4    f(1, loop)\n",
    "    5    ...\n",
    "\n",
    "Call-by-name, CBN:\n",
    "\n",
    "    1    f(1, loop)\n",
    "    2    1\n",
    "    \n",
    "Scala uses **CBV** and **CBN** whenever the argument as the tokens **=>**. Example:\n",
    "\n",
    "    1    f(1, loop)\n",
    "    2    f(1, loop)\n",
    "    3    ...\n",
    "    4    f(1, loop)\n",
    "    5    ...\n",
    "    \n",
    "    1    g(1, loop)\n",
    "    2    1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties.\n",
    "# Call-by-value and Call-by-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties.\n",
    "# Conditional Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Algorithms\n",
    "\n",
    "Algorithms are a finite series of well-defined steps to reduce an expression to a value. The expression is reasoning of a solution to a problem, the value is the solution for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Newton's Method\n",
    "\n",
    "Newton's algorithm for finding root solutions for real valued functions.\n",
    "\n",
    "## Implementations\n",
    "\n",
    "[Mathematical](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
    "\n",
    "**Scala**:\n",
    "```scala\n",
    "def sqrt(root: Double): Double = {\n",
    "\n",
    "  def inRadius(guess: Double) : Boolean = abs(guess * guess - root) / root < 0.0001\n",
    "\n",
    "  def approach(guess: Double): Double = ((root / guess) + guess) / 2\n",
    "\n",
    "  def sqrNewton(guess: Double) : Double =\n",
    "    if (inRadius(guess)) guess\n",
    "    else sqrNewton(approach(guess))\n",
    "\n",
    "  // the last element defines the value\n",
    "  sqrNewton(1.0)\n",
    "\n",
    "} + 0\n",
    "```\n",
    "\n",
    "    \n",
    "**Disadvantages**:\n",
    "For *r* $\\in [0, 1]$, (*guess* \\* *guess*) $\\longrightarrow$ 0 $\\Longrightarrow$ (*guess* \\* *guess* - *r*) $\\longrightarrow$ *r* $\\geq$ *radius*\n",
    "\n",
    "For very large *r*, there will be **overflow**.\n",
    "\n",
    "Therefore the *inRadius* test is made relatively to *root*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Euclid's GCD\n",
    "\n",
    "## Implementations\n",
    "\n",
    "[Mathematical](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
    "\n",
    "**Scala**:\n",
    "```scala\n",
    "def factorial (x: Double): Double = {\n",
    "  def rec (x: Double, acc: Double): Double =\n",
    "    if (x <= 0) acc\n",
    "    else rec(x - 1, acc * x)\n",
    "  rec(x, 1.0)\n",
    "}\n",
    "```\n",
    "\n",
    "    \n",
    "**Disadvantages**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Newton's Binomium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Factorial\n",
    "\n",
    "## Implementations\n",
    "\n",
    "[Mathematical]()\n",
    "\n",
    "**Scala**:\n",
    "```scala\n",
    "def sqrt(root: Double): Double = {\n",
    "\n",
    "  def inRadius(guess: Double) : Boolean = abs(guess * guess - root) / root < 0.0001\n",
    "\n",
    "  def approach(guess: Double): Double = ((root / guess) + guess) / 2\n",
    "\n",
    "  def sqrNewton(guess: Double) : Double =\n",
    "    if (inRadius(guess)) guess\n",
    "    else sqrNewton(approach(guess))\n",
    "\n",
    "  // the last element defines the value\n",
    "  sqrNewton(1.0)\n",
    "\n",
    "} + 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties\n",
    "# Recursion and Tail Recursion\n",
    "\n",
    "Recursion uses a method's **stack frame** (limited up to 1000 in the JVM). Whenever a method is deeply recursive, use **Tail Recursion**: reuse the stack frame of the method.\n",
    "```scala\n",
    "import scala.annotation.tailrec\n",
    "@tailrec\n",
    "def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ai'></a>\n",
    "[/](#root)\n",
    "\n",
    "-----\n",
    "# Artificial Intelligence\n",
    "\n",
    "AI are complex algorithms one can say to be *intelligent*.\n",
    "\n",
    "Problems tackled by AI:\n",
    "\n",
    "+ **Binary Classification** | Deciding **whether or not** an object satisfies a given condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='machine_learning'></a>\n",
    "[/ Artificial Intelligence](#ai)\n",
    "\n",
    "-----\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "Machine learning is a field of AI, consisting of a scientific study of algorithms and statistical models that computer systems use to improve their performance on certain tasks. ML algorithms model mathematicaly a **sample data**/**training set** in order to expratolate/learn information so that they can make decisions without having been explicitly told how to. There are cases where it is **unfeasible to develop algorithms with specific instructions** (computer vision, etc.), and that's where ML algorithms excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deep_learning'></a>\n",
    "[/ Artificial Intelligence / Machine Learning](#machine_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "Key terms: <font color=\"blue\">training dataset, test dataset, neural network, activation function, layered neural network, neural network architecture, gradient descent</font>\n",
    "\n",
    "Deep learning is a field of machine learning. It extrapolates information from data by **training the algorithm** from a **training dataset**. For small training datasets, traditional learning algorithms were sufficient. For large training datasets, neural networks excelled. Deep learning neural networks involve chosing from:\n",
    "+ neural network architecture,\n",
    "+ activation functions,\n",
    "+ argument initialization,\n",
    "+ etc.\n",
    "\n",
    "It is difficult to find guidelines on how to tackle a specific problem, deep learning is also doing research in these related fields.\n",
    "\n",
    "## Contents\n",
    "\n",
    "+ Neural Networks [$\\rightleftharpoons$](#neural_networks)\n",
    "    + Recurrent Neural Networks\n",
    "    + Standard Neural Networks\n",
    "    + Convolutional Neural Networks\n",
    "+ Supervised Learning [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Binary Classification [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Logistic Regression [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Cost and Loss Functions [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Gradient Descent [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Computation Graph [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Logistic Regression and Gradient Descent [$\\rightleftharpoons$](#deep_learning)\n",
    "+ Vectorization [$\\rightleftharpoons$](#deep_learning)\n",
    "\n",
    "<a id='neural_networks'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning](#machine_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "A NN is modelled as a **layered directed graph**. Its' vertices are **transformations** $t$ (also called **activation functions**), it's edges are I/O relations between the vertices. $l = 0$ is the ***input layer***, $l = L$ is the ***output layer***. Vertices are organized in layers $0 \\geq l \\leq L$. $n^{[l]}$ is the number of activations in the layer $l$. \n",
    "\n",
    "Neural networks are modelled with **layered oriented graphs**. A neural network with **N** hidden layers, will have as parameters $W^{[1]},\\,b^{[1]}$, ..., $W^{[i]},\\,b^{[i]}$, ..., $W^{[N]},\\,b^{[N]}$, each corresponding to a layer $i$.\n",
    "\n",
    "\n",
    "There are:\n",
    "\n",
    "+ Recurrent Neural Networks\n",
    "+ Standard Neural Networks\n",
    "+ Convolutional Neural Networks\n",
    "\n",
    "<a id='layered_neural_networks'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Neural Networks](#deep_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Layered Neural Networks\n",
    "$W$ changes according to the *architecture* of the neural network, that is to say, according to the number of nodes per layer, each layer.\n",
    "\n",
    "<a id='supervised_learning'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning](#machine_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Supervised Learning\n",
    "\n",
    "Learning is supervised when there is a **labelled training set**. Images are encoded into vectors. Iteratively, optimal values for W and b for all images of the training set are found. By then, the test set can be labelled.\n",
    "\n",
    "<a id='supervised_learning_algorithms'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning](#supervised_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Supervised Learning Algorithms\n",
    "\n",
    "+ Logistic Regression\n",
    "\n",
    "\n",
    "<a id='logistic_regression'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms](#supervised_learning_algorithms)\n",
    "\n",
    "-----\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "\n",
    "LOGR is a **supervised deep learning algorithm** tackling the problem of **Binary Classification**. It trains a neural network on a **training set**, and then heads to classify items on a test set. Let the BC problem rest over a training set $TS$ of images. \n",
    "\n",
    "The algoritms performs as:\n",
    "+ Preprocessing the sample data\n",
    "+ Building the neural network\n",
    "+ Finding the solution/Training\n",
    "+ Classifying\n",
    "\n",
    "<a id='prepo'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "# Preprocessing the sample data\n",
    "\n",
    "Fix dimensions for the training set (encoded sample data) $TS$ and for the encoded images:\n",
    "+ #$TS = M$\n",
    "+ training images dimensions $n,\\,m: TS = \\mathbb{N}^{m\\,*\\,n\\,*\\,3}$\n",
    "\n",
    "The image encoding is the mapping:\n",
    "\n",
    "$$image \\longrightarrow X \\in \\mathbb{N}^{m\\,*\\,n\\,*\\,3} = TS\\tag{1}$$\n",
    "\n",
    "Each image pixel is a triple of RGB values.\n",
    "\n",
    "Classify the encoded images, the **training set** $TS$:\n",
    "\n",
    "$$ TS = \\{(x^{(i)}, y^{(i)}) : i \\leq M\\}\\tag{2}$$\n",
    "\n",
    "$y^{(i)}$, the correct classification of $x^{(i)}$. \n",
    "\n",
    "<a id='build_nn'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "\n",
    "# Building the neural network\n",
    "\n",
    "A NN is a function with an **absolute minimum** (The absolute minimum is the solution that will prompt the LOGR algorithm to perform the classification). A NN is a composite function. As such, a NN can be extended to it's composite functions and dependencies between the composite functions. Composite functions are often called **Activation Functions**.\n",
    "\n",
    "This arrange is depicted in the **NN Architecture** and modelled as an **oriented graph** or **computation graph** [$\\rightleftharpoons$](#computation_graph). Each function is represented as a node, and the dependencies are represented by incoming/outgoing arches, function's input/ouput.\n",
    "\n",
    "The graph is organized in layers. Layer 0 is the **input layer**, the last layer is the **output layer**. The layers in between are the **hidden Layers**. They're called *hidden* because their IO values are never seen.\n",
    "\n",
    "## Constants formatting and initialization\n",
    "\n",
    "If $m=n$, $TS = \\mathbb{N}^{n_x}$, $n_x = n_{x^{(i)}}$, $x^{(i)} \\in TS$, $i \\leq M$, an encoding of an image $i$ in $TS$. The $M$ encodings are assembled in the **matrix** $X$:\n",
    "\n",
    "$$X \\in M_{n_x\\,*\\,M} (\\mathbb{N})  = \n",
    "\\begin{bmatrix}\n",
    "    x_{1}^1 & \\dots & x_{1}^M \\\\\n",
    "    x_{2}^1 & \\dots & x_{2}^M \\\\\n",
    "    \\dots & \\dots & \\dots \\\\\n",
    "    x_{nn3}^1 & \\dots & x_{nn3}^M\n",
    "\\end{bmatrix}\n",
    "\\tag{3}$$\n",
    "\n",
    "$x^{(i)}$ coordinates are distributed along the $i$-th column of $X$.\n",
    "\n",
    "## Input formatting and initialization\n",
    "\n",
    "The execution of the NN computes **estimations**. The encodings and the correct classifications are fixed constants in the NN expression, thus the estimation is a function of $w$, or <font color=\"red\">for some reason (W1)</font>, a function of $W, b$.\n",
    "\n",
    "<font color=\"red\">For some reason (W3) </font>$W^{[i]}$ cannot be initialized to 0 values due to the **simmetry problem** (Logistic regression’s weights w should be initialized randomly rather than to all zeros, because if you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to “break symmetry”? No, Logistic Regression doesn't have a hidden layer. If you initialize the weights to zeros, the first example x fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input x (because there's no hidden layer) which is not zero. So at the second iteration, the weights values follow x's distribution and are different from each other if x is not a constant vector.). , $b^{[i]}$ is safe to be initialized with 0. $W^{[i]}$ are initialized to **random values**.\n",
    "\n",
    "## Output formatting\n",
    "\n",
    "The classification is the mapping:\n",
    "\n",
    "$$LOGR : x^{(i)} \\in TS \\longrightarrow \\hat{y}^{(i)} \\in \\{0, 1\\} \\tag{1}$$\n",
    "\n",
    "The classifications of $X$ are assembled in the **vector** $Y$:\n",
    "\n",
    "$$ Y = (\\hat{y}^{(1)}\\,.\\,.\\,.\\,\\hat{y}^{(M)}) \\tag{4}$$\n",
    "\n",
    "\n",
    "<a id='training'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "\n",
    "# Training\n",
    "\n",
    "The training over $(x^{(i)}, y^{(i)}) \\in TS$ performs as follows:\n",
    "\n",
    "1. **Estimation** of the classification $\\hat{y}^{(i)}$ for $x^{(i)}$\n",
    "1. Compute the **loss** of $\\hat{y}^{(i)}$ in relation to $y^{(i)}$\n",
    "1. Compute the **cost** $\\mathcal{C}$ of all the estimations $\\hat{y}^{(i)}$ in relation to $y^{(i)}$\n",
    "4. Minimize $\\mathcal{C}$\n",
    "1. Found $\\mathcal{C}$ minimum ? **STOP** : **GO TO 1.**\n",
    "\n",
    "It is desired that the **cost** be minimal, so the cost function $\\mathcal{C}$ is **minimized**. The minimizing algorithm of LR is the **Gradient Descent**. Minimizing $\\mathcal{C}$ attains new values $W,b$. These values are in fact **improving** the estimates as $\\mathcal{C}$ is minimized, for this means that the $\\mathcal{C}$ $\\longrightarrow 0$, and so that $\\hat{y}^{(i)} \\longrightarrow \\hat{y}^{(i)}$. This improving is thought of as **learning**. Optimizing values for $W,b$ is learning values for  $W,b$ that correctly classify the encodings.\n",
    "\n",
    "<a id='estimation'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training](#training)\n",
    "\n",
    "-----\n",
    "\n",
    "# Estimation\n",
    "\n",
    "The neural network **estimates** whether $x^{(i)}$ is correctly labelled by $y^{(i)}$. <font color=\"red\">For some reason</font>, the function is computed as a linear combination $W^TX + b$, which is then <font color=\"red\">for some reason related to y being 0 or 1</font> reduced to boundaries $[0, 1]$ through a transformation $\\sigma$ that must satisfy the conditions:\n",
    "\n",
    "$$\\sigma(x) \\in [0, 1]\\tag{3}$$\n",
    "$$\\lim_{x \\to +\\infty} \\sigma(x) \\longrightarrow 1\\tag{4}$$\n",
    "$$\\lim_{x \\to -\\infty} \\sigma(x) \\longrightarrow 0\\tag{5}$$\n",
    "\n",
    "A solution for $\\sigma$:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}\\tag{6}$$\n",
    "\n",
    "$\\hat{Y} = (\\hat{y}^{(1)},\\,.\\,.\\,.,\\,\\hat{y}^{(M)})$ is thus attained as: \n",
    "\n",
    "$$ \\hat{Y} = \\sigma(W^TX + b) = \\frac{1}{1 + e^{-(W^TX + b)}}\\tag{7}$$\n",
    "\n",
    "where each estimation is:\n",
    "\n",
    "$$ \\hat{y}^{(i)} = \\sigma(W^Tx^{(i)} + b) = \\frac{1}{1 + e^{-(W^Tx^{(i)} + b)}}\\tag{8}$$\n",
    "\n",
    "\n",
    "The $\\sigma$ function: [mathematical](https://www.wolframalpha.com/input/?i=1+%2F+(1+%2B+e%5E(-x) | **python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14ac295a278>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14ac278b320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xx = np.arange(-5., 5., .1)\n",
    "plt.text(.0, .9, r'$\\sigma(x)$', fontsize=20)\n",
    "plt.plot(xx, sigma(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cost_loss_functions'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training](#training)\n",
    "\n",
    "-----\n",
    "\n",
    "# Loss and Cost\n",
    "\n",
    "$\\mathcal{C}$ is the function intended to minimize, therefore $\\mathcal{C}$ must satisfy the condition ([$\\rightleftharpoons$](#functions_properties), otherwise an optimal solution $(W, b)$ might not be found):\n",
    "\n",
    "$$\\mathcal{C} \\:be\\:convex\\tag{1}$$\n",
    "\n",
    "Let $\\mathcal{C}$ be a mean of the estimations loss:\n",
    "\n",
    "$$\\mathcal{C}(W, b) := \\frac{1}{M}\\sum_{1}^{M}\\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\\tag{2}$$\n",
    "\n",
    "Given that a sum of convex functions is a convex function, let's request that $\\mathcal{L}$ satisfies the condition:\n",
    "\n",
    "$$\\mathcal{L} \\:be\\:convex\\tag{3}$$\n",
    "\n",
    "It is also requested that the loss function $\\mathcal{L}$ satisfies that: \n",
    "\n",
    "$$\\hat{y} \\longrightarrow y \\Longleftrightarrow \\mathcal{L}(\\hat{y}, y) \\longrightarrow 0 \\tag{4.1}$$\n",
    "$$\\hat{y} \\longrightarrow 1 - y \\Longleftrightarrow \\mathcal{L}(\\hat{y}, y) \\longrightarrow +\\infty \\tag{4.1}$$\n",
    "\n",
    "The relation between the estimation and the correct classification can be modelled through a **measure of probability** $P(y\\, |\\, x)$:\n",
    "\n",
    "$$\n",
    "P(y\\, |\\, x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    \\hat{y} & , y = 1\\\\\n",
    "    1 - \\hat{y} & , y = 0\\\\\n",
    "\\end{array}\\right. = \\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)} \\tag{5}$$\n",
    "\n",
    "If we let $\\mathcal{L}(\\hat{y}, y) = \\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)}$, $\\mathcal{L}$ would not satisfy conditions $(4.1)$, $(4.2)$. However by applying $log$ to $\\mathcal{L}$ (safe to perform for $log$ is stricly monotonic, thus optimizing $\\mathcal{L}$ is optimizing $log$ o $\\mathcal{L}$), we get:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y) := log(P(y\\, |\\, x)) = log(\\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)}) = ylog(\\hat{y}) + (1 - y)log(1 - \\hat{y})\\tag{6}$$\n",
    "\n",
    "From which we get for $y = 0$: \n",
    "   \n",
    "$$\\lim_{\\hat{y} \\to 0} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 0} log(1 - \\hat{y}) = log(1) = 0 \\tag{7.1}$$\n",
    "$$\\lim_{\\hat{y} \\to 1} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 1} log(1 - \\hat{y}) = \\lim_{x \\to 0} log(x) = -\\infty \\tag{7.2}$$\n",
    "\n",
    "and for $y = 1$:\n",
    "\n",
    "$$\\lim_{\\hat{y} \\to 0} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 0} log(\\hat{y}) = -\\infty \\tag{7.3}$$\n",
    "$$\\lim_{\\hat{y} \\to 1} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 1} log(\\hat{y}) = log(1) = 0 \\tag{7.4}$$\n",
    "\n",
    "A factor of $-1$ enforces the desired behaviour:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y) := -log(P(y\\, |\\, x)) = -log(\\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)}) = (y - 1)log(1 - \\hat{y}) - ylog(\\hat{y}) \\tag{8}$$\n",
    "\n",
    "\n",
    "Equation $(2)$ extends to:\n",
    "\n",
    "$$\\mathcal{C}(W, b) := -\\frac{1}{M}\\sum_{1}^{M}log((\\frac{1}{1 + e^{W^Tx+b}})^{y}(1 - \\frac{1}{1 + e^{W^Tx+b}})^{(1-y)})$$\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14ac2950278>,\n",
       " <matplotlib.lines.Line2D at 0x14ac2950a20>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14ac2c4cf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss(x, y):\n",
    "    return np.log((sigma(x)**y)*(1-y)**(1-sigma(x)))\n",
    "\n",
    "xx = np.arange(-5., 5., .1)\n",
    "yy = np.arange(-5., 5., .1)\n",
    "plt.plot(xx, yy, loss(xx, yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gradient_descent'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training](#training)\n",
    "\n",
    "-----\n",
    "\n",
    "# Minimizing the cost\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "**Gradient descent** is the algorithm for minimizing the overall cost $\\mathcal{C}$ for all the estimations, thus **learning** new and optimal values for $W, b$, *i.e.*, the minimum of $\\mathcal{C}$. The algorithm performs the following steps:\n",
    "\n",
    "<br/>\n",
    "\n",
    "For the current values $W, b$:\n",
    "1. Get $\\mathcal{C}$ (the **Forward Propagation** step)\n",
    "1. Get $\\frac{\\partial}{\\partial u_i}\\mathcal{C}$ (the **Backward Propagation** step)\n",
    "1. The derivative increases ?\n",
    "    1. [true] GO BACK (went too far)\n",
    "    1. [false] Update values, CONTINUE\n",
    "\n",
    "<br/>\n",
    "\n",
    "For $u_i \\in \\{w_{1},\\,...,w_{3mn}, b\\}$, $\\alpha \\geq 0$ (the *rate of descent*), $\\mathcal{G}$ (the **computation graph** for LOGR):\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"img/binary_classification_neural_network_without_cost\" alt=\"Computation Graph\"/>\n",
    "<br/>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "Fig.1 | Neural network architecture for LOGR with$n\\,*\\,n\\,*\\,3\\,+\\,1$ inputs and 3 nodes:\n",
    "<br/>\n",
    "$$W^TX+b\\tag{A}$$\n",
    "$$\\sigma(A)\\tag{B}$$\n",
    "$$\\mathcal{L}(W, b)\\tag{C}$$\n",
    "</figcaption>\n",
    "\n",
    "## Get cost $\\mathcal{C}$\n",
    "\n",
    "$$\\mathcal{C}(u_1,\\,...,u_i,\\,...,u_{3mn + 1})\\tag{1}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$(A)$ is computed and its' output **A** is fed onto $(B)$, which in turn feeds its' output **B** to $(C)$. The simetric of the average $\\mathcal{C}$ of **C**, the output of $(C)$ is then computed. In the example, $M$ 2\\*2 images are processed to update values for $W, b$:\n",
    "\n",
    "VERSION 1  \n",
    "**pseudo** | python | scala\n",
    "```python\n",
    "C = 0, dw1 = 0, dw2 = 0, db = 0\n",
    "\n",
    "for i = 1 to M:\n",
    "\tzi = wTxi + b\n",
    "\tai = sigma(zi)\n",
    "\tC += -(yi*log(ai) + (1 - yi)log(1 - ai))\n",
    "\tdzi = ai - yi\n",
    "\tdw1 += x1i*dzi\n",
    "\tdw2 += x2i*dzi\n",
    "\tdb += dzi\n",
    "C /= M, dw1 /= M, dw2 /= M, db /= M\n",
    "\n",
    "w1 = w1 - alpha*dw1\n",
    "w2 = w2 - alpha*dw2\n",
    "b = b - alpha*db\n",
    "```\n",
    "\n",
    "VERSION 2  \n",
    "pseudo | **python** | scala\n",
    "```python\n",
    "for i in range(ITER_NR):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigma(Z)\n",
    "    dZ = A - Y\n",
    "    dW = 1/M * X * dZ.T\n",
    "    db = 1/M * np.sum(dZ)\n",
    "    W = W - alpha * dW\n",
    "    b = b - alpha * db\n",
    "```\n",
    "\n",
    "## Get differentiation values\n",
    "\n",
    "$$\\frac{\\partial}{\\partial u_i}\\mathcal{L}(u_1,\\,...,u_i,\\,...,u_{3mn + 1})\\tag{2}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Differentiate $(C)$ in respect to **B**. Differentiate $(C)$ in respect to **A**. Differentiate $(C)$ in respect to $u_i$. Multiply the values (chain rule) and get the differentiation for the each of the current values $u_i$.\n",
    "\n",
    "## Update values in case the differentiation is negative\n",
    "\n",
    "$$u_i := u_i - \\alpha \\frac{\\partial}{\\partial u_i}\\mathcal{L}(u_1,\\,...,u_i,\\,...,u_{3mn + 1})\\tag{3}$$\n",
    "\n",
    "\n",
    "A single iteration of the gradient descent:\n",
    "\n",
    "pseudo | **python** | scala\n",
    "\n",
    "```python\n",
    "# forward\n",
    "Z = np.dot(W,X) + b\n",
    "A = sigma(Z)\n",
    "\n",
    "# backward\n",
    "dZ = A - Y\n",
    "dW = dW = ((1 / M) * X * dZ).sum(axis = 1).reshape(len(X), 1)\n",
    "db = (1 / M) * np.sum(dZ)  \n",
    "\n",
    "# update values\n",
    "W -= alpha * dW\n",
    "b -= alpha * db\n",
    "```\n",
    "\n",
    "<a id='rate_of_descent'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training / Minimizing: Gradient Descent](#gradient_descent)\n",
    "\n",
    "-----\n",
    "\n",
    "# Rate of descent\n",
    "\n",
    "The rate of descent determines $\\alpha$ how steep the gradient descent performs. If $\\alpha$ is too high, the cost function can oscilate and even diverge. If it is too small, too many iterations may be required to reach the minimum and the learning curve will ve very flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='computation_graph'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training / Minimizing: Gradient Descent](#gradient_descent)\n",
    "\n",
    "-----\n",
    "# Computation Graph\n",
    "\n",
    "The neural network for binary classification consists of four nodes and $n\\,*\\,n\\,*\\,3\\,+\\,1$ inputs for $W, b$:\n",
    "\n",
    "$$W^TX+b\\tag{A}$$\n",
    "$$\\sigma(A)\\tag{B}$$\n",
    "$$\\mathcal{L}(W, b)\\tag{C}$$\n",
    "$$\\mathcal{C}(X, B)\\tag{D}$$\n",
    "\n",
    "\n",
    "<img src=\"img/binary_classification_neural_network_without_cost\" alt=\"Computation Graph\"/>\n",
    "\n",
    "$(D)$ is left out considering it does only take the simetric of the average of all loss.\n",
    "\n",
    "<a id='classification'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "\n",
    "## Classifying\n",
    "### Input formatting\n",
    "\n",
    "$TS* = \\mathbb{N}^{n_x}$, $n_x = n_{x^{(i)}}$, $x^{(i)} \\in TS$, $i \\leq M$, an encoding of an image $i$ in $TS*$. The $M$ encodings are assembled in the **matrix** $X$:\n",
    "\n",
    "$$X \\in M_{n_x\\,*\\,M} (\\mathbb{N})  = \n",
    "\\begin{bmatrix}\n",
    "    x_{1}^1 & \\dots & x_{1}^M \\\\\n",
    "    x_{2}^1 & \\dots & x_{2}^M \\\\\n",
    "    \\dots & \\dots & \\dots \\\\\n",
    "    x_{nn3}^1 & \\dots & x_{nn3}^M\n",
    "\\end{bmatrix}\n",
    "\\tag{3}$$\n",
    "\n",
    "$x^{(i)}$ coordinates are distributed along the $i$-th column of $X$.\n",
    "\n",
    "### Output formatting\n",
    "\n",
    "The classification is the mapping:\n",
    "\n",
    "$$LR : x^{(i)} \\in TS \\longrightarrow \\hat{y}^{(i)} \\in \\{0, 1\\} \\tag{1}$$\n",
    "\n",
    "The classifications of $X$ are assembled in the **vector** $Y$:\n",
    "\n",
    "$$ Y = (\\hat{y}^{(1)}\\,.\\,.\\,.\\,\\hat{y}^{(M)}) \\tag{4}$$\n",
    "\n",
    "Classifying (**python**):\n",
    "\n",
    "```python\n",
    "# upon optimal solution\n",
    "A = sigmoid(np.dot(w.T, X) + b)\n",
    "for i in range(A.shape[1]):\n",
    "    Y[0][i] = 0 if (A[0][i] < 0.5) else 1\n",
    "```\n",
    "\n",
    "<a id='activation_functions'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning](#deep_learning)\n",
    "\n",
    "-----\n",
    "# Activation Functions\n",
    "\n",
    "<font color=\"red\">For some reason</font> the choice of activation functions is very important. Usually activation functions are indexed by layer, $g^{[i]}$ will be the activation function for all nodes/activations of layer $i$. <font color=\"red\">For some reason</font>, it is convenient that $g^{[i]}$ are not **linear transformations** (in which case, <font color=\"red\">for some reason</font> all $g^{[i]}$ will be equivalent to a single linear transformation $g$, and the whole NN architecture will have no more expressiveness than a single activation $g$).\n",
    "\n",
    "## Tips for chosing $g$\n",
    "\n",
    "\n",
    "$tanh \\subseteq [-1,\\,1]$ centers the data giving the data a 0 mean. It makes the learnign easier to the layer ahead. In binary classification it is better to use this function in the hidden layers, using $\\sigma \\subseteq [0,\\,1]$ only in the output layer. Different activation functions $g$ can be applied to different layers $i$, $g^{[i]}$.\n",
    "\n",
    "Problems with $tanh$, $\\sigma$: For $x \\longrightarrow \\pm \\infty$, the activation functions always evaluate to a close to zero derivative, which slows down the gradient descent. The rectified linear unit or $RELU(x) = max(0,\\,x)$ tackles this issue by having derivative 1 for $x\\,>\\,0$, and derivative 0 for $x\\,<\\,0$. Although undefined for $x\\,=\\,0$, it is very unlikely that the gradient descent takes on 0. The $Leaky\\,RELU(x) = max(0.01x,\\,x)$ copes with the 0 derivative of $RELU$.\n",
    "\n",
    "Some problems require activations in $\\mathbb{R}$. In this case, the output layer can be a linear activator, whereas the hidden layers can be $Leaky\\,RELU$ or $RELU$.\n",
    "A linear transformation can be used though in the output layer, when the output is a $r \\in \\mathbb{R}$.\n",
    "\n",
    "<a id='overfitting'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning](#deep_learning)\n",
    "\n",
    "-----\n",
    "# Overfitting\n",
    "\n",
    "\n",
    "Increasing too much the train accuracy decreases the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
