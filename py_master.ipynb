{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Notebook for Mathematics, Computer Science and Machine Learning\n",
    "Diogo PEREIRA MARQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Introduction\n",
    "\n",
    "The present notebook presents and explains important ideas for Computer Science and Machine learning.\n",
    "\n",
    "It accompanies this explanation with the **computer programming ** and **mathematical implementation** of these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Supported languages\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg\" alt=\"drawing\" width=\"200\"/> <img src=\"https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='root'></a>\n",
    "-----\n",
    "\n",
    "# /\n",
    "\n",
    "+ Function Definition\n",
    "+ **Data** [$\\rightleftharpoons$](#data)\n",
    "    + Data Structures [$\\rightleftharpoons$](#data_structures)\n",
    "    + Data Types and Operators [$\\rightleftharpoons$](#data_types)\n",
    "        + Simple\n",
    "            + Numbers\n",
    "            + Words\n",
    "        + Complex\n",
    "            + Image [$\\rightleftharpoons$](#image)\n",
    "            + Vectors and Matrices\n",
    "                + Addition [$\\rightleftharpoons$](#vect_mat_addition)\n",
    "                + Addition \\* [$\\rightleftharpoons$](#vect_mat_addition_star)\n",
    "                + Product [$\\rightleftharpoons$](#vect_mat_product)\n",
    "                + Product \\* [$\\rightleftharpoons$](#vect_mat_product_star)\n",
    "                + Division [$\\rightleftharpoons$](#vect_mat_division)\n",
    "                + Reshape [$\\rightleftharpoons$](#vect_mat_reshape)\n",
    "                + Map, Filter and Reduce [$\\rightleftharpoons$](#vect_mat_map_filter_reduce)\n",
    "            + Sets and Classes\n",
    "            + Algebra\n",
    "                + Composite [$\\rightleftharpoons$](#composite)\n",
    "                + Functions and Properties [$\\rightleftharpoons$](#functions_properties)\n",
    "                + Random variable [$\\rightleftharpoons$](#random_variable)\n",
    "                + Probability [$\\rightleftharpoons$](#probability)\n",
    "                    + Conditional Probability [$\\rightleftharpoons$](#conditional_probability)\n",
    "                + Differentiation [$\\rightleftharpoons$](#differentiation)\n",
    "            + Time\n",
    "        + Lexic\n",
    "        + Data Type Equivalence Operations\n",
    "+ **Computer Programming Language**\n",
    "    + Local File System Access\n",
    "    + Method Definition\n",
    "        + Remarks\n",
    "    + Computer Application/Program\n",
    "        + Application Structure\n",
    "            + Packages\n",
    "    + Properties [$\\rightleftharpoons$](#prog_properties)\n",
    "        + Lazy and Greedy Evaluation\n",
    "        + Class Hierarchy\n",
    "        + List Comprehension\n",
    "        + Curry Functions\n",
    "        + Indentation sensibility\n",
    "        + Readability\n",
    "        + Complexity and Performance [$\\rightleftharpoons$](#complexity_performance)\n",
    "        + Mutability and Immutability\n",
    "        + Side-effect Operations\n",
    "        + Substitution Model\n",
    "        + Evaluation strategies: Call-by-value and Call-by-name\n",
    "        + Call-by-value and Call-by-reference\n",
    "        + Conditional expressions\n",
    "        + Recursion and Tail Recursion\n",
    "        + Objects\n",
    "        + Exception Handling\n",
    "+ **Algorithms and Computer Science** [$\\rightleftharpoons$](#algorithms_cs)\n",
    "+ **Artificial Intelligence** [$\\rightleftharpoons$](#ai)\n",
    "    + Machine Learning [$\\rightleftharpoons$](#machine_learning)\n",
    "        + Deep Learning [$\\rightleftharpoons$](#deep_learning)\n",
    "            + Neural Networks [$\\rightleftharpoons$](#neural_networks)\n",
    "                + Recurrent Neural Networks\n",
    "                + Standard Neural Networks\n",
    "                + Convolutional Neural Networks\n",
    "                + Layered Neural Networks [$\\rightleftharpoons$](#layered_neural_networks)\n",
    "            + Supervised Learning [$\\rightleftharpoons$](#deep_learning)\n",
    "                + Binary Classification [$\\rightleftharpoons$](#binary_classification)\n",
    "                    + Logistic Regression [$\\rightleftharpoons$](#logistic_regression)\n",
    "                    + Cost and Loss Functions [$\\rightleftharpoons$](#cost_loss_functions)\n",
    "                    + Gradient Descent [$\\rightleftharpoons$](#gradient_descent)\n",
    "                        + Rate of descent [$\\rightleftharpoons$](#rate_of_descent)\n",
    "                        + Forward and Backward Propagation [$\\rightleftharpoons$](#fwd_bwd_propagation)\n",
    "                    + Testing the Binary Classification [$\\rightleftharpoons$](#test_binary_classification)\n",
    "            + Activation Functions [$\\rightleftharpoons$](#activation_functions)\n",
    "            + Overfitting [$\\rightleftharpoons$](#overfitting)\n",
    "+ **Meta**\n",
    "    + New Markdown Cell\n",
    "    + Code Block\n",
    "    + Maths and Latex\n",
    "    + HTML\n",
    "    + Kernel Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "[/](#root)\n",
    "\n",
    "-----\n",
    "\n",
    "# Data\n",
    "\n",
    "Data can be either **structured** or **unstructured**. Human processors are excellent at extrapolating information from the undestructured sort, mechanic processors from the structured sort. Structured data has a **data type**, and data types are either **simple** or **complex**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_types'></a>\n",
    "[/ Data](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Data Types\n",
    "\n",
    "*Entities* belong to *simple* or *complex* data types. Elements of **NUMBERS** represent entities called *quantities*. Numbers from $\\mathbb{R}$ to $\\mathbb{C}$ represent **continuous** quantities. All other data types sets represent **discrete** entities: $\\mathbb{Z}$, $\\mathbb{Q}$, **STRING**, etc.\n",
    "\n",
    "\n",
    "Only **human processors** can understand continuous quantities, **mecanic processors** can only understand discrete quantities.\n",
    "\n",
    "<a id='data_type_closure'></a>\n",
    "[/ Data / Data Types](#data_types)\n",
    "\n",
    "-----\n",
    "\n",
    "# Closure\n",
    "\n",
    "All elements of a given data type $\\mathbb{D}$ are **closed under certain operations $\\mathbb{O}$**, *i.e.*, each element of $\\mathbb{D}$ is equivalent to *n* elements of $\\mathbb{D}$, related to one another through some *n*-ary relation/operation $\\odot \\in \\mathbb{O}$. \n",
    "\n",
    "Typically $\\mathbb{D}$ is refered to both by its' elements and by its' relations $\\mathbb{O}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='symbol'></a>\n",
    "[/ Data / Data Types](#data_types)\n",
    "\n",
    "-----\n",
    "\n",
    "# Symbolic Representation\n",
    "\n",
    "Each *data type* has it's own *symbolic representation*, which in turn has it's own *lexic of symbols*.\n",
    "Symbols can be either **alpha-numerical** (characters) or **numerical** (digits). Sequences of digits are called **numbers**, sequences of characters are called **words**.\n",
    "\n",
    "<a id='symbol'></a>\n",
    "[/ Data / Data Types / Symbolic Representation](#symbol)\n",
    "\n",
    "-----\n",
    "\n",
    "# Lexic\n",
    "\n",
    "The lexic is composed of elements of the set **CHARACTERS** and the NULL element $\\epsilon$, the *symbol without symbol*.\n",
    "\n",
    "+ **CHARACTERS** = $\\mathbb{UNICODE}$ $\\bigcup \\{ \\epsilon \\}$ = { [unicode](https://unicode-table.com/en/) } $\\bigcup \\{ \\epsilon \\}$\n",
    "\n",
    "+ **NUMERICAL_CHARACTERS** = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }\n",
    "\n",
    "+ **ALPHA_NUMERICAL_CHARACTERS** = **CHARACTERS** - **NUMERICAL_CHARACTERS**\n",
    "\n",
    "\n",
    "Note that:\n",
    "\n",
    "+ Characters **do not** represent data. 1 represents the natural number 1, '1' is the character used to represent the natural number 1. As we see in this example, neither the characters nor the integers are not closed under the operation of **concatenation, +**, whilst the integers are closed under the operation **addition, +**.\n",
    "\n",
    "\n",
    "\n",
    "<center>**1** = '1' + $\\epsilon$</center>\n",
    "\n",
    "<center>**1 + 1** = '1' + ' ' + '+' + '1' = **2**</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = '1'\n",
    "b = 1\n",
    "\n",
    "print(\n",
    "    a == b\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Some characters convey meaning not about the *fact* that the word represents, but meaning about **how to display the word** to the word interpreter. These characters are usually refered to as **white-space** characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "1\t1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"11\")\n",
    "print(\"1\\t1\")\n",
    "print(\"1\\n1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Data Types > Simple Data Types\n",
    "\n",
    "# Boolean Values\n",
    "\n",
    "**BOOLEAN** = { *true*, *false* }. Elements of BOOLEAN represent the concepts of *true* and *false*. BOOLEAN is closed under:\n",
    "+ Equality, $=$\n",
    "+ Inequality, $\\neq$\n",
    "+ Disjunction, $\\bigvee$\n",
    "+ Conjunction, $\\bigwedge$\n",
    "+ Negation, \n",
    "\n",
    "BOOLEAN elements are usually equivalent to **expression evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Data Types.Simple Data Types.\n",
    "# Numbers\n",
    "\n",
    "\n",
    "\n",
    "+ $\\mathbb{N}$, **natural** numbers: positive integers\n",
    "\n",
    "+ $\\mathbb{Z}$, **integer** numbers: negative and positive integers and 0\n",
    "\n",
    "+ $\\mathbb{Q}$, **rational** numbers: finite decimal value, or periodic infinite decimal value\n",
    "\n",
    "+ $\\mathbb{R}$, **real** numbers: rational and irrational numbers\n",
    "\n",
    "+ $\\mathbb{C}$, **complex** numbers: real and imaginary numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rational Numbers, $\\mathbb{Q}$ | PY\n",
    "\n",
    "[operators](https://docs.python.org/2/library/math.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Data Types.Complex Data Types.\n",
    "# String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image'></a>\n",
    "[/ Data / Data Types / Complex](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Image\n",
    "\n",
    "Images are $m \\in \\mathbb{M}_{h x l}^3 ([0, 255])$, where $h, l$ are the height and the length of the image, and the $m_{i, j}$ are the Red, Green and Blue values. Typically images are **encoded** to columnar vectors $m \\in \\mathbb{M}_{h x l x 3} ([0, 255])$\n",
    "\n",
    "<a id='functions_properties'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Functions and Properties\n",
    "\n",
    "| | | |\n",
    "| --- | --- | --- |\n",
    "| |[convex](http://mathworld.wolfram.com/ConvexFunction.html) | |\n",
    "| [log](http://mathworld.wolfram.com/Logarithm.html) | yes | | \n",
    "| [tanh](http://mathworld.wolfram.com/HyperbolicTangent.html) |  | | \n",
    "\n",
    "<a id='composite'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "# Composite\n",
    "\n",
    "$$f_{1\\,.\\,.\\,n} (x) := (f_{1} \\circ \\cdots \\circ f_{n})(x)\\tag{1}$$\n",
    "$$f_{1\\,.\\,.\\,1} = f_1\\tag{2}$$\n",
    "$$f_{1\\,.\\,.\\,a}, a < 1 = id\\tag{3}$$\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "# Random Variable\n",
    "\n",
    "<a id='probability'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "# Random Variable\n",
    "\n",
    "\n",
    "Random variables are functions $P$ atributing a measure of probability to $X$. $P$ is **discrete** if $X$ is a number, $P$ is **continuous** if $X$ is an interval.\n",
    "\n",
    "$$P : \\Omega \\longrightarrow \\mathbb{R} \\tag{1}$$\n",
    "\n",
    "<a id='probability'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Randomness and Probability\n",
    "\n",
    "Attribute **weights** $w_i$  to all elements whithin a set. If $w_i = w_j, i \\neq j$, then the set represents a **random event**, and this is considered as **randomness**. Weights attributed to elements whithin an event, or to an event are called **measure of probability**.\n",
    "\n",
    "<a id='conditional_probability'></a>\n",
    "[/ Data / Data Types / Algebra / Function](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Conditional Probability\n",
    "\n",
    "Probability of the event $E_1$ given the probability of an event $E_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='differentiation'></a>\n",
    "[/ Data / Data Types / Algebra /](#data)\n",
    "\n",
    "-----\n",
    "\n",
    "# Differentiation\n",
    "\n",
    "**Composite** FINISH\n",
    "$$f_{1\\,.\\,.\\,n}'(x) = f_1' \\left( f_{2\\,.\\,.\\,n}(x) \\right) \\; f_2' \\left( f_{3\\,.\\,.\\,n}(x) \\right) \\cdots f_{n-1}' \\left(f_{n\\,.\\,.\\,n}(x)\\right) \\; f_n'(x) = \\prod_{k=1}^{n} f_k' \\left(f_{(k+1\\,.\\,.\\,n)}(x) \\right)\\tag{1}$$\n",
    "\n",
    "$$f_{1\\,.\\,.\\,n}'(x) = f_1' \\left( u_1 \\right) \\; u_1' \\left( u_2 \\right) \\cdots u_{n-2}' \\left(u_{n-1}\\right) \\; u_{n-1}'(x) = \\prod_{k=1}^{n} f_k' \\left(f_{(k+1\\,.\\,.\\,n)}(x) \\right)\\tag{2}$$\n",
    "\n",
    "$$ u_{i} = f_{i + 1\\,.\\,.\\,n}\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Series\n",
    "\n",
    "[mathematical]()\n",
    "\n",
    "**python**\n",
    "\n",
    "Series are **NumPy.Arrays** of arrays, or arrays, although the **first have more operations**.\n",
    "\n",
    "\n",
    "**Properties**\n",
    "+ Enumerable\n",
    "\n",
    "\n",
    "\n",
    "Let:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Series.\n",
    "# Operations | PY\n",
    "\n",
    "+ list comprehension\n",
    "+ [further]()\n",
    "\n",
    "# List comprehension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Series.\n",
    "# Useful | PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗∗ c ∗∗ d, a) is to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Sets\n",
    "\n",
    "[mathematical]()\n",
    "\n",
    "**python**\n",
    "\n",
    "Sets are **Set** of **NumPy.Array** or arrays.\n",
    "\n",
    "**Properties**\n",
    "+ Unordered\n",
    "+ Without repetition\n",
    "\n",
    "Let **S1** such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat'></a>\n",
    "[Data / Data Types and Operators / Complex ]()\n",
    "\n",
    "-----\n",
    "\n",
    "# Matrices\n",
    "\n",
    "[mathematical](http://mathworld.wolfram.com/Matrix.html) | **python**\n",
    "\n",
    "Matrices are specific sort of **Series**.\n",
    "\n",
    "**Properties**\n",
    "+ Its' elements $e$ are also series (but not recursively)\n",
    "+ len($e_i$) = len($e_j$), $i \\neq j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_ops'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices](#vect_mat)\n",
    "\n",
    "-----\n",
    "# Operations\n",
    "\n",
    "<a id='transpose'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Transpose\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='len_size'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "\n",
    "# Len, Size\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_addition'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Addition\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_addition_star'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops) \n",
    "\n",
    "-----\n",
    "# Addition*\n",
    "\n",
    "~~mathematical~~ | **python**\n",
    "\n",
    "**Addition\\*** is defined as:\n",
    "\n",
    "$$(R, C) + (1, 1) \\longrightarrow (R, C)\\tag{1}$$\n",
    "$$(R, C) + (R, 1) \\longrightarrow (R, C)\\tag{2}$$\n",
    "$$(R, C) + (1, C) \\longrightarrow (R, C)\\tag{3}$$\n",
    "$$(R, C) + (R, C) \\longrightarrow (R, C)\\tag{4}$$\n",
    "$$(R, C) + a \\longrightarrow (R, C)\\tag{5}$$\n",
    "\n",
    "Expression $(2)$ adds a column to all columns. Expression $(3)$ adds a row to all rows. Expression $(4)$ is the regular matrix addition operation. Expression $(1)$ adds a column to all columns.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_product'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)  \n",
    "\n",
    "-----\n",
    "# Product\n",
    "\n",
    "+ Inner Product\n",
    "+ Outer Product\n",
    "+ Elementwise Product\n",
    "+ Scalar Product\n",
    "\n",
    "[mathematical]() | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_product_star'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Product*\n",
    "\n",
    "~~mathematical~~ | **python**\n",
    "\n",
    "**Product\\*** is defined as:\n",
    "\n",
    "$$(R, C) * (1, 1) \\longrightarrow (R, C)\\tag{1}$$\n",
    "$$(R, C) * (1, C) \\longrightarrow (R, C)\\tag{2}$$\n",
    "$$(R, C) * (R, 1) \\longrightarrow (R, C)\\tag{3}$$\n",
    "$$(R, C) * (C, R*) \\longrightarrow (R, R*)\\tag{4}$$\n",
    "\n",
    "Expression $(2)$ multiplies each column with the column. Expression $(3)$ multiplies each row with the row. Expression $(1)$ performs the scalar product. Expression $(4)$ performs the inner product.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_division'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Division\n",
    "\n",
    "**Division** is defined as:\n",
    "\n",
    "$$(R, C) \\,/\\, a \\longrightarrow (R, C)\\tag{1}$$\n",
    "$$(R, C) \\,/\\, (1, 1) \\longrightarrow (1, 1)\\tag{2}$$\n",
    "$$(R, C) \\,/\\, (1, C) \\longrightarrow (R, C)\\tag{3}$$\n",
    "$$(R, C) \\,/\\, (R, 1) \\longrightarrow (R, C)\\tag{4}$$\n",
    "$$(R, C) \\,/\\, (R, C) \\longrightarrow (R, C)\\tag{5}$$\n",
    "    \n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_reshape'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Reshape\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_map_filter_reduce'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Map, Filter and Reduce\n",
    "\n",
    "An example to get the sum of all the even powers of two up to 100.\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_mat_reshape'></a>\n",
    "[Data / Data Types and Operators / Complex / Vectores and Matrices / Operations](#vect_mat_ops)\n",
    "\n",
    "-----\n",
    "# Sum\n",
    "\n",
    "~~mathematical~~ | **python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **shape** returns a **rank 1** list. Reshaped it to a **non-rank 1** matrice. Numerous bugs arrive from rank 1 lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primitive Operators\n",
    "\n",
    "Arithmetic python scala\n",
    "\n",
    "Logical python scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Method Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f1(a1, a2):\n",
    "    return a1, a2\n",
    "\n",
    "def f2(a1):\n",
    "    return f1(a, a + a)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f2(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_structures'></a>\n",
    "[/ Data](#data)\n",
    "\n",
    "-----\n",
    "# Data Structures\n",
    "\n",
    "Data has a data type. A data type has at least one data structure. We'll suppose for now that it has **one and one only** data structure. We can try and generalize that the data structure of a data type $\\mathbb{D}$ is **how** elements of $\\mathbb{D}$ can be obtained with elements of $\\mathbb{D}*$ through operations of $\\mathbb{O}$.\n",
    "\n",
    "<code>\"string\" = 's' + 't' + 'r' + 'i' + 'n' + 'g' </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple:\n",
      "(1, 2, 3)\n",
      "Triple's elements:\n",
      "1 2 3\n",
      "\n",
      "Dictionary:\n",
      "{'x': 1, 'y': 2, 'z': 3}\n",
      "Dictionary's keys:\n",
      "x y z\n",
      "Dictionary's values:\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "triple = (1,2,3)\n",
    "dictio = {'x' : 1, 'y' : 2, 'z' : 3}\n",
    "\n",
    "print(\n",
    "    \"Triple:\\n\"\n",
    "    + str(triple)\n",
    "    + \"\\nTriple's elements:\\n\"\n",
    "    + (lambda x, y, z : str(x) + \" \" + str(y) + \" \" + str(z))(*triple)\n",
    "    \n",
    "    + \"\\n\\nDictionary:\\n\"\n",
    "    + str(dictio)\n",
    "    + \"\\nDictionary's keys:\\n\"\n",
    "    + (lambda x, y, z : str(x) + \" \" + str(y) + \" \" + str(z))(*dictio)\n",
    "    + \"\\nDictionary's values:\\n\"\n",
    "    + (lambda x, y, z : str(x) + \" \" + str(y) + \" \" + str(z))(**dictio)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Function Definition\n",
    "\n",
    "Lambda functions can't use regular python statements and always include an implicit return statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f3 = lambda x, y: (lambda x : 2 * x)(f2(x + y)) # Non-anonymous and anonymous lambda expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f3(1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Definition.Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-b764db2f52c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "a = np.random.rand(1000000000)\n",
    "b = np.random.rand(1000000000)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "np.dot(a,b) # transpose and multiply\n",
    "\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf = t1 - t0\n",
    "print(\"time spent on vectorization: \" + str(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "acc = 0\n",
    "for i in range(1000000000):\n",
    "    acc += a[i] * b[i]\n",
    "    \n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Time spent in non-vectorization\" + str(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array(\n",
    "    [[56.0, 0.0, 4.4, 68.0]\n",
    "    , [1.2, 104.0, 52.0, 8.0]\n",
    "    , [1.8, 135.0, 99.0, 0.9]]\n",
    ")\n",
    "\n",
    "calories = arr.sum(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( 100 * arr / calories.reshape(1, 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use RANK1 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='complexity_performance'></a>\n",
    "[/ Computer Programming Language / Properties](#prog_properties)\n",
    "\n",
    "-----\n",
    "\n",
    "# Complexity and Performance\n",
    "\n",
    "# For-loops\n",
    "\n",
    "Python evades embedded for-loops with **vectorization**, by allowing non-mathematical operations on matrices such as **Addition\\***, **Multiplication\\***, **Division**. Vectorization enables (for example) an implementation of a single elevation of gradient descent with respect to an entire training set without using even a singel explicit for-loop.\n",
    "\n",
    "-----\n",
    "Computer Programming Language\n",
    ".Properties\n",
    "# .Readability\n",
    "\n",
    "Readability is a property of computer applications with direct implications in properties such as **Scalability** and **Maintenance**. Readability is enforced by [good practices](https://code.tutsplus.com/tutorials/top-15-best-practices-for-writing-super-readable-code--net-8118).\n",
    "\n",
    "Properties such as **Application Structure** also influence properties such as **Scalability** and **Maintenance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language\n",
    ".Properties\n",
    "# .Mutability and Immutability\n",
    "\n",
    "Mutability is **change that does not affect the identity**. Immutability is **equivalence between state and identity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language\n",
    ".Properties\n",
    "# .Substitution Model\n",
    "\n",
    "The **substitution model** is formalized in the $\\lambda$-calculus (a foundation for functional programming). Every expression computes to a value. This model can express every algorithm, being equivalent to a **Turing Machine**.\n",
    "\n",
    "Expressions with **side-effects** cannot be represented by the $\\lambda$-calculus.</p>\n",
    "**Example:**\n",
    "```java\n",
    "c = 1\n",
    "System.out.println(c++);\n",
    "System.out.println(c);\n",
    "```\n",
    "\n",
    "**Output**   \n",
    "\n",
    "1 </p>\n",
    "2\n",
    "\n",
    "Do all expressions reduce to values in the SModel? No.</p>\n",
    "**Example:**\n",
    "\n",
    "```scala    \n",
    "def loop: Int = loop\n",
    "loop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties.\n",
    "# Evaluation strategies: Call-by-value and Call-by-name\n",
    "\n",
    "**Call-by-value, CBV**:\n",
    "\n",
    "    1    sumOfSquares(3, 2 + 2)\n",
    "    2    sumOfSquares(3, 4)\n",
    "    3    square(3) + square(4)\n",
    "    4    3 * 3 + square(4)\n",
    "    5    3 * 3 + 4 * 4\n",
    "    6    9 + 4 * 4\n",
    "    7    9 + 16\n",
    "    8    25\n",
    "    \n",
    "**Advantages**: Evaluates a function argument only once (Call-by-name, steps 5-6: performs *2 + 2* twice). Usually exponentially more efficient than CBN.\n",
    "    \n",
    "**Call-by-name, CBN**:\n",
    "\n",
    "    1    sumOfSquares(3, 2 + 2)\n",
    "    2    square(3) + square(2 + 2)\n",
    "    3    3 * 3 + square(2 + 2)\n",
    "    4    9 + square(2 + 2)\n",
    "    5    9 + (2 + 2) + (2 + 2) \n",
    "    6    9 + 4 + (2 + 2)\n",
    "    7    9 + 4 * 4\n",
    "    8    9 + 16\n",
    "    9    25\n",
    "\n",
    "**Advantages**: A function argument is not evaluated if it is not used in the evaluation. \n",
    "\n",
    "Both strategies **evaluate an expression to a value** *iff*: \n",
    "+ reduced expressions consist only of *pure functions* and \n",
    "+ all evaluations terminate.\n",
    "\n",
    "**Properties**:\n",
    "+ Evaluation **terminates** $\\Longleftrightarrow$ Reduced expressions consist only of *pure functions* $\\bigwedge$ all evaluations terminate\n",
    "+ CBV terminates $\\Longrightarrow$ CBN terminates\n",
    "\n",
    "Example:\n",
    "\n",
    "```scala\n",
    "def f(x: Int, y: Int) : Int = x\n",
    "def g(x: Int, y: => Int) : Int = x\n",
    "```\n",
    "\n",
    "Call-by-value, CBV:  \n",
    "    \n",
    "    1    f(1, loop)\n",
    "    2    f(1, loop)\n",
    "    3    ...\n",
    "    4    f(1, loop)\n",
    "    5    ...\n",
    "\n",
    "Call-by-name, CBN:\n",
    "\n",
    "    1    f(1, loop)\n",
    "    2    1\n",
    "    \n",
    "Scala uses **CBV** and **CBN** whenever the argument as the tokens **=>**. Example:\n",
    "\n",
    "    1    f(1, loop)\n",
    "    2    f(1, loop)\n",
    "    3    ...\n",
    "    4    f(1, loop)\n",
    "    5    ...\n",
    "    \n",
    "    1    g(1, loop)\n",
    "    2    1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties.\n",
    "# Call-by-value and Call-by-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties.\n",
    "# Conditional Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Algorithms\n",
    "\n",
    "Algorithms are a finite series of well-defined steps to reduce an expression to a value. The expression is reasoning of a solution to a problem, the value is the solution for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Newton's Method\n",
    "\n",
    "Newton's algorithm for finding root solutions for real valued functions.\n",
    "\n",
    "## Implementations\n",
    "\n",
    "[Mathematical](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
    "\n",
    "**Scala**:\n",
    "```scala\n",
    "def sqrt(root: Double): Double = {\n",
    "\n",
    "  def inRadius(guess: Double) : Boolean = abs(guess * guess - root) / root < 0.0001\n",
    "\n",
    "  def approach(guess: Double): Double = ((root / guess) + guess) / 2\n",
    "\n",
    "  def sqrNewton(guess: Double) : Double =\n",
    "    if (inRadius(guess)) guess\n",
    "    else sqrNewton(approach(guess))\n",
    "\n",
    "  // the last element defines the value\n",
    "  sqrNewton(1.0)\n",
    "\n",
    "} + 0\n",
    "```\n",
    "\n",
    "    \n",
    "**Disadvantages**:\n",
    "For *r* $\\in [0, 1]$, (*guess* \\* *guess*) $\\longrightarrow$ 0 $\\Longrightarrow$ (*guess* \\* *guess* - *r*) $\\longrightarrow$ *r* $\\geq$ *radius*\n",
    "\n",
    "For very large *r*, there will be **overflow**.\n",
    "\n",
    "Therefore the *inRadius* test is made relatively to *root*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Euclid's GCD\n",
    "\n",
    "## Implementations\n",
    "\n",
    "[Mathematical](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
    "\n",
    "**Scala**:\n",
    "```scala\n",
    "def factorial (x: Double): Double = {\n",
    "  def rec (x: Double, acc: Double): Double =\n",
    "    if (x <= 0) acc\n",
    "    else rec(x - 1, acc * x)\n",
    "  rec(x, 1.0)\n",
    "}\n",
    "```\n",
    "\n",
    "    \n",
    "**Disadvantages**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Newton's Binomium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Algorithms.\n",
    "\n",
    "# Factorial\n",
    "\n",
    "## Implementations\n",
    "\n",
    "[Mathematical]()\n",
    "\n",
    "**Scala**:\n",
    "```scala\n",
    "def sqrt(root: Double): Double = {\n",
    "\n",
    "  def inRadius(guess: Double) : Boolean = abs(guess * guess - root) / root < 0.0001\n",
    "\n",
    "  def approach(guess: Double): Double = ((root / guess) + guess) / 2\n",
    "\n",
    "  def sqrNewton(guess: Double) : Double =\n",
    "    if (inRadius(guess)) guess\n",
    "    else sqrNewton(approach(guess))\n",
    "\n",
    "  // the last element defines the value\n",
    "  sqrNewton(1.0)\n",
    "\n",
    "} + 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Computer Programming Language.Properties\n",
    "# Recursion and Tail Recursion\n",
    "\n",
    "Recursion uses a method's **stack frame** (limited up to 1000 in the JVM). Whenever a method is deeply recursive, use **Tail Recursion**: reuse the stack frame of the method.\n",
    "```scala\n",
    "import scala.annotation.tailrec\n",
    "@tailrec\n",
    "def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ai'></a>\n",
    "[/](#root)\n",
    "\n",
    "-----\n",
    "# Artificial Intelligence\n",
    "\n",
    "AI are complex algorithms one can say to be *intelligent*.\n",
    "\n",
    "Problems tackled by AI:\n",
    "\n",
    "+ **Binary Classification** | Deciding **whether or not** an object satisfies a given condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='machine_learning'></a>\n",
    "[/ Artificial Intelligence](#ai)\n",
    "\n",
    "-----\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "Machine learning is a field of AI, consisting of a scientific study of algorithms and statistical models that computer systems use to improve their performance on certain tasks. ML algorithms model mathematicaly a **sample data**/**training set** in order to expratolate/learn information so that they can make decisions without having been explicitly told how to. There are cases where it is **unfeasible to develop algorithms with specific instructions** (computer vision, etc.), and that's where ML algorithms excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deep_learning'></a>\n",
    "[/ Artificial Intelligence / Machine Learning](#machine_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "Key terms: <font color=\"blue\">training dataset, test dataset, neural network, activation function, layered neural network, neural network architecture, gradient descent</font>\n",
    "\n",
    "Deep learning is a field of machine learning. It extrapolates information from data by **training the algorithm** from a **training dataset**. For small training datasets, traditional learning algorithms were sufficient. For large training datasets, neural networks excelled. Deep learning neural networks involve chosing from:\n",
    "+ neural network architecture,\n",
    "+ activation functions,\n",
    "+ argument initialization,\n",
    "+ etc.\n",
    "\n",
    "It is difficult to find guidelines on how to tackle a specific problem, deep learning is also doing research in these related fields.\n",
    "\n",
    "<a id='neural_networks'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning](#machine_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## Layered directed graphs\n",
    "\n",
    "A NN is modelled as a **layered directed graph**. Its' vertices are **transformations** $t$ (also called **activation functions**), it's edges are I/O relations between the vertices. Vertices are organized in layers $0 \\leq l \\leq L$. $l = 0$ is the *input layer*, $l = L$ is the *output layer*. Layers $0 < l < L$ are the *hidded layers*.\n",
    "\n",
    "+ $a_i^{[l]}$, the $i-th$ activation of layer $l$.\n",
    "+ $a^{[l]}$, the activations of layer $l$.\n",
    "+ $g^{[l]}$, the activation function of layer $l$.\n",
    "+ $n^{[0]} = n_x$, the number of activations in the input layer: the dimension of each element in the training set $TS$ ($a_i^{[0]} = id$).\n",
    "+ $n^{[l]}$, the number of activations in layer $l$.\n",
    "+ $n^{[0]} = n_x$, the number of activations in the input layer: the dimension of each element in the training set $TS$.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "Each layer of a NN takes as parameters:\n",
    "\n",
    "+ ($W^{[1]},\\,b^{[1]}$), ..., ($W^{[L]},\\,b^{[L]}$), one pair by layer, the *weights* and the *bias*.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Parameters that influence the performance of the NN:\n",
    "\n",
    "+ The learning rate $\\alpha$\n",
    "+ The number of iterations of the gradient descent algorithm\n",
    "+ $L$\n",
    "+ $n^{[l]}$\n",
    "+ $g^{[l]}$\n",
    "\n",
    "\n",
    "## Types of NN\n",
    "\n",
    "+ Recurrent Neural Networks\n",
    "+ Standard Neural Networks\n",
    "+ Convolutional Neural Networks\n",
    "\n",
    "\n",
    "<a id='supervised_learning'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning](#machine_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Supervised Learning\n",
    "\n",
    "Learning is supervised when there is a **labelled training set**.\n",
    "\n",
    "<a id='supervised_learning_algorithms'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning](#supervised_learning)\n",
    "\n",
    "-----\n",
    "\n",
    "# Supervised Learning Algorithms\n",
    "\n",
    "+ Logistic Regression\n",
    "\n",
    "\n",
    "<a id='logistic_regression'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms](#supervised_learning_algorithms)\n",
    "\n",
    "-----\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "\n",
    "LOGR is a **supervised deep learning algorithm** tackling the problem of **Binary Classification**. It trains a neural network on a *training set* $TS$, and then heads to classify items on a *test set* $TS*$. Let $TS$ be a set of *encoded images*. The purpose is to *learn* to recognize a binary feature on the elements of $TS$.\n",
    "\n",
    "The algoritms performs as:\n",
    "1. Preprocessing the sample data\n",
    "1. Building the neural network\n",
    "1. Finding the solution/Training (iterative process)\n",
    "1. Classifying\n",
    "\n",
    "<a id='prepo'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "# Preprocessing the sample data\n",
    "\n",
    "Fix dimensions for $TS$ and for the images $img \\in TS$:\n",
    "+ #$TS = M$\n",
    "+ training images dimensions $n_x \\in \\mathbb{N}^{m\\,*\\,n}$\n",
    "\n",
    "## Encoding\n",
    "\n",
    "The image encoding is the mapping:\n",
    "\n",
    "$$img \\longrightarrow X \\in \\mathbb{N}^{m\\,*\\,n\\,*\\,3} = TS\\tag{1}$$\n",
    "\n",
    "The mapping occurs by processing an image as a matrix $\\mathcal{M}_{m\\,*\\,n}(\\mathbb{N}^3)$. The scalars of the matrix are triples  $\\in \\mathbb{N}^3$ (the RGB values $\\in \\{0,\\,...,\\,255\\}$). The matrix is then flattened to a vector $x \\in \\mathcal{M}_{1\\,*\\,3mn}(\\mathbb{N})$. The $M$ $x^{(i)}$ coordinates of the encodings are distributed along the $i$-th column of the matrix $X$:\n",
    "\n",
    "$$X \\in M_{n_x\\,*\\,M} (\\mathbb{N})  = \n",
    "\\begin{bmatrix}\n",
    "    x_{1}^{(1)} & \\dots & x_{1}^{(M)} \\\\\n",
    "    x_{2}^{(1)} & \\dots & x_{2}^{(M)} \\\\\n",
    "    \\dots & \\dots & \\dots \\\\\n",
    "    x_{n_x}^{(1)} & \\dots & x_{n_x}^{(M)}\n",
    "\\end{bmatrix}\n",
    "\\tag{2}$$\n",
    "\n",
    "\n",
    "\n",
    "## Labelling\n",
    "\n",
    "Each vector $x^{(i)}$ is classified with a label y^{(i)}, and thus the $TS$ is defined as: \n",
    "\n",
    "$$ TS = \\{(x^{(i)}, y^{(i)}) : i \\leq M\\}\\tag{3}$$\n",
    "\n",
    "\n",
    "<a id='build_nn'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "\n",
    "# Building the neural network\n",
    "\n",
    "Together, the choices for the values below define the *NN architecture*.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "For $1 \\leq i \\leq L$, random initialization for:\n",
    "\n",
    "+ $W^{[i]},\\,b^{[i]}$\n",
    "\n",
    "$$W^{[i]} \\in \\mathcal{M}_{n^{[i]}\\,*\\,n^{[i\\,-\\,1]}}(\\mathbb{R}) \\tag{1}$$\n",
    "$$b^{[i]} \\in \\mathbb{R} \\tag{2}$$\n",
    "\n",
    "<font color=\"red\">For some reason (W3) </font>$W^{[i]}$ cannot be initialized to 0 values due to the **simmetry problem** (Logistic regression’s weights w should be initialized randomly rather than to all zeros, because if you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to “break symmetry”? No, Logistic Regression doesn't have a hidden layer. If you initialize the weights to zeros, the first example x fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input x (because there's no hidden layer) which is not zero. So at the second iteration, the weights values follow x's distribution and are different from each other if x is not a constant vector.). , $b^{[i]}$ is safe to be initialized with 0. $W^{[i]}$ are initialized to **random values**.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Values for:\n",
    "\n",
    "+ The learning rate $\\alpha$\n",
    "+ The number of iterations of the gradient descent algorithm\n",
    "+ $L$\n",
    "+ $n^{[l]}$\n",
    "+ $g^{[l]}$\n",
    "\n",
    "Each activation is defined as:\n",
    "\n",
    "$$g^{[l]}(W^{[l]}X^T + b^{[l]}) \\tag{3}$$\n",
    "\n",
    "\n",
    "\n",
    "## Chosing $g$\n",
    "\n",
    "<font color=\"red\">For some reason</font>, it is convenient that $g^{[i]}$ are not **linear transformations** (in which case, <font color=\"red\">for some reason</font> all $g^{[i]}$ will be equivalent to a single linear transformation $g$, and the whole NN architecture will have no more expressiveness than a single activation $g$). \n",
    "\n",
    "$tanh \\subseteq [-1,\\,1]$ centers the data giving the data a 0 mean. It makes the learnign easier to the layer ahead. In binary classification it is better to use this function in the hidden layers, using $\\sigma \\subseteq [0,\\,1]$ only in the output layer. Different activation functions $g$ can be applied to different layers $i$, $g^{[i]}$.\n",
    "\n",
    "Problems with $tanh$, $\\sigma$: For $x \\longrightarrow \\pm \\infty$, the activation functions always evaluate to a close to zero derivative, which slows down the gradient descent. The rectified linear unit or $RELU(x) = max(0,\\,x)$ tackles this issue by having derivative 1 for $x\\,>\\,0$, and derivative 0 for $x\\,<\\,0$. Although undefined for $x\\,=\\,0$, it is very unlikely that the gradient descent takes on 0. The $Leaky\\,RELU(x) = max(0.01x,\\,x)$ copes with the 0 derivative of $RELU$.\n",
    "\n",
    "Some problems require activations in $\\mathbb{R}$. In this case, the output layer can be a linear activator, whereas the hidden layers can be $Leaky\\,RELU$ or $RELU$.\n",
    "A linear transformation can be used though in the output layer, when the output is a $r \\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "<a id='training'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "\n",
    "# Training\n",
    "\n",
    "The training over $(x^{(i)}, y^{(i)}) \\in TS$ performs as follows:\n",
    "\n",
    "1. **Estimation** of the classification $\\hat{y}^{(i)}$ for $x^{(i)}$\n",
    "1. Compute the **loss** of $\\hat{y}^{(i)}$ in relation to $y^{(i)}$\n",
    "1. Compute the **cost** $\\mathcal{C}$ of all the estimations $\\hat{y}^{(i)}$ in relation to $y^{(i)}$\n",
    "4. Minimize $\\mathcal{C}$\n",
    "1. Found $\\mathcal{C}$ minimum ?\n",
    "    1. [true] get $W^{[i]},\\,b^{[i]}$\n",
    "    1. [false] continue\n",
    "\n",
    "It is desired that the **cost** be minimal, so the cost function $\\mathcal{C}$ is **minimized**. The minimizing algorithm of LOGR is the **Gradient Descent**. Minimizing $\\mathcal{C}$ attains new values $W,b$. These values are in fact **improving** the estimates as $\\mathcal{C}$ is minimized, for this means that the $\\mathcal{C}$ $\\longrightarrow 0$, and so that $\\hat{y}^{(i)} \\longrightarrow \\hat{y}^{(i)}$. This improving is thought of as **learning**. Optimizing values for $W^{[i]},\\,b^{[i]}$ is learning values for  $W^{[i]},\\,b^{[i]}$ that correctly classify the encodings.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "\n",
    "Increasing too much the train accuracy decreases the test accuracy.\n",
    "\n",
    "<a id='estimation'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training](#training)\n",
    "\n",
    "-----\n",
    "\n",
    "# Estimation\n",
    "\n",
    "The neural network **estimates** whether $x^{(i)}$ is correctly labelled by $y^{(i)}$. <font color=\"red\">For some reason</font>, the function is computed as a linear combination $W^TX + b$, which is then <font color=\"red\">for some reason related to y being 0 or 1</font> reduced to boundaries $[0, 1]$ through a transformation $\\sigma$ that must satisfy the conditions:\n",
    "\n",
    "$$\\sigma(x) \\in [0, 1]\\tag{3}$$\n",
    "$$\\lim_{x \\to +\\infty} \\sigma(x) \\longrightarrow 1\\tag{4}$$\n",
    "$$\\lim_{x \\to -\\infty} \\sigma(x) \\longrightarrow 0\\tag{5}$$\n",
    "\n",
    "A solution for $\\sigma$:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}\\tag{6}$$\n",
    "\n",
    "$\\hat{Y} = (\\hat{y}^{(1)},\\,.\\,.\\,.,\\,\\hat{y}^{(M)})$ is thus attained as: \n",
    "\n",
    "$$ \\hat{Y} = \\sigma(W^{[L - 1]}X^T + b^{[L - 1]}) = \\frac{1}{1 + e^{-(W^{[L\\,-\\,1]}X^T + b^{[L\\,-\\,1]})}}\\tag{7}$$\n",
    "\n",
    "where each estimation is:\n",
    "\n",
    "$$ \\hat{y}^{(i)} = \\sigma(W^{[L - 1]}x^{(i)} + b^{[L - 1]})) = \\frac{1}{1 + e^{-(W^{[L\\,-\\,1]}x^{(i)} + b^{[L\\,-\\,1]})}}\\tag{8}$$\n",
    "\n",
    "\n",
    "The $\\sigma$ function: [mathematical](https://www.wolframalpha.com/input/?i=1+%2F+(1+%2B+e%5E(-x) | python | scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cost_loss_functions'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training](#training)\n",
    "\n",
    "-----\n",
    "\n",
    "# Loss and Cost\n",
    "\n",
    "\n",
    "$\\mathcal{C}$ is the function intended to minimize, therefore $\\mathcal{C}$ must satisfy the condition ([$\\rightleftharpoons$](#functions_properties), otherwise an optimal solution $(W, b)$ might not be found):\n",
    "\n",
    "$$\\mathcal{C} \\:be\\:convex\\tag{1}$$\n",
    "\n",
    "Let $\\mathcal{C}$ be a mean of the estimations loss:\n",
    "\n",
    "$$\\mathcal{C}(W, b) := \\frac{1}{M}\\sum_{1}^{M}\\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\\tag{2}$$\n",
    "\n",
    "Given that a sum of convex functions is a convex function, let's request that $\\mathcal{L}$ satisfies the condition:\n",
    "\n",
    "$$\\mathcal{L} \\:be\\:convex\\tag{3}$$\n",
    "\n",
    "It is also requested that the loss function $\\mathcal{L}$ satisfies that: \n",
    "\n",
    "$$\\hat{y} \\longrightarrow y \\Longleftrightarrow \\mathcal{L}(\\hat{y}, y) \\longrightarrow 0 \\tag{4.1}$$\n",
    "$$\\hat{y} \\longrightarrow 1 - y \\Longleftrightarrow \\mathcal{L}(\\hat{y}, y) \\longrightarrow +\\infty \\tag{4.1}$$\n",
    "\n",
    "The relation between the estimation and the correct classification can be modelled through a **measure of probability** $P(y\\, |\\, x)$:\n",
    "\n",
    "$$\n",
    "P(y\\, |\\, x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    \\hat{y} & , y = 1\\\\\n",
    "    1 - \\hat{y} & , y = 0\\\\\n",
    "\\end{array}\\right. = \\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)} \\tag{5}$$\n",
    "\n",
    "If we let $\\mathcal{L}(\\hat{y}, y) = \\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)}$, $\\mathcal{L}$ would not satisfy conditions $(4.1)$, $(4.2)$. However by applying $log$ to $\\mathcal{L}$ (safe to perform for $log$ is stricly monotonic, thus optimizing $\\mathcal{L}$ is optimizing $log$ o $\\mathcal{L}$), we get:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y) := log(P(y\\, |\\, x)) = log(\\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)}) = ylog(\\hat{y}) + (1 - y)log(1 - \\hat{y})\\tag{6}$$\n",
    "\n",
    "From which we get for $y = 0$: \n",
    "   \n",
    "$$\\lim_{\\hat{y} \\to 0} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 0} log(1 - \\hat{y}) = log(1) = 0 \\tag{7.1}$$\n",
    "$$\\lim_{\\hat{y} \\to 1} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 1} log(1 - \\hat{y}) = \\lim_{x \\to 0} log(x) = -\\infty \\tag{7.2}$$\n",
    "\n",
    "and for $y = 1$:\n",
    "\n",
    "$$\\lim_{\\hat{y} \\to 0} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 0} log(\\hat{y}) = -\\infty \\tag{7.3}$$\n",
    "$$\\lim_{\\hat{y} \\to 1} \\mathcal{L}(\\hat{y}, y) = \\lim_{\\hat{y} \\to 1} log(\\hat{y}) = log(1) = 0 \\tag{7.4}$$\n",
    "\n",
    "A factor of $-1$ enforces the desired behaviour:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y) := -log(P(y\\, |\\, x)) = -log(\\hat{y}^y\\,(1 - \\hat{y})^{(1 - y)}) = (y - 1)log(1 - \\hat{y}) - ylog(\\hat{y}) \\tag{8}$$\n",
    "\n",
    "\n",
    "Equation $(2)$ extends to:\n",
    "\n",
    "$$\\mathcal{C}(W, b) := -\\frac{1}{M}\\sum_{1}^{M}log((\\frac{1}{1 + e^{W^Tx+b}})^{y}(1 - \\frac{1}{1 + e^{W^Tx+b}})^{(1-y)})$$\n",
    "\n",
    "[mathematical]() | **python**\n",
    "\n",
    "```python\n",
    "def loss(x, y):\n",
    "    return np.log((sigma(x)**y)*(1-y)**(1-sigma(x)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gradient_descent'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training](#training)\n",
    "\n",
    "-----\n",
    "\n",
    "# Minimizing the cost\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "**Gradient descent** is the algorithm for minimizing the overall cost $\\mathcal{C}$ for all the estimations, thus **learning** new and optimal values for $W, b$, *i.e.*, the minimum of $\\mathcal{C}$. The algorithm performs the following steps:\n",
    "\n",
    "<br/>\n",
    "\n",
    "For the current values $W^{[l]}, b^{[l]}$:\n",
    "1. Get $\\mathcal{C}$ (the **Forward Propagation** step)\n",
    "1. Get $\\frac{\\partial}{\\partial u_i}\\mathcal{C}$ (the **Backward Propagation** step)\n",
    "1. The derivative increases ?\n",
    "    1. [true] GO BACK (went too far)\n",
    "    1. [false] Update values, CONTINUE\n",
    "\n",
    "<br/>\n",
    "\n",
    "For $u_i \\in \\{w_1^1,\\,...,w_{n^{[l]}}^{n^{[l\\,-\\,1]}}, b^{[l]}\\}$, $\\alpha \\geq 0$ (the *rate of descent*), $\\mathcal{G}$ (the **computation graph** for LOGR):\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"img/binary_classification_neural_network_without_cost\" alt=\"Computation Graph\"/>\n",
    "<br/>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "Fig.1 | Neural network architecture for LOGR with $n_x$ inputs and 3 nodes:\n",
    "<br/>\n",
    "$$W^TX+b\\tag{A}$$\n",
    "$$\\sigma(A)\\tag{B}$$\n",
    "$$\\mathcal{L}(W, b)\\tag{C}$$\n",
    "</figcaption>\n",
    "\n",
    "## Get cost $\\mathcal{C}$\n",
    "\n",
    "$$\\mathcal{C}(u_1,\\,...,u_i,\\,...,u_{3mn + 1})\\tag{1}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$(A)$ is computed and its' output **A** is fed onto $(B)$, which in turn feeds its' output **B** to $(C)$. The simetric of the average $\\mathcal{C}$ of **C**, the output of $(C)$ is then computed. In the example, $M$ 2\\*2 images are processed to update values for $W, b$:\n",
    "\n",
    "VERSION 1  \n",
    "**pseudo** | python | scala\n",
    "```python\n",
    "C = 0, dw1 = 0, dw2 = 0, db = 0\n",
    "\n",
    "for i = 1 to M:\n",
    "\tzi = wTxi + b\n",
    "\tai = sigma(zi)\n",
    "\tC += -(yi*log(ai) + (1 - yi)log(1 - ai))\n",
    "\tdzi = ai - yi\n",
    "\tdw1 += x1i*dzi\n",
    "\tdw2 += x2i*dzi\n",
    "\tdb += dzi\n",
    "C /= M, dw1 /= M, dw2 /= M, db /= M\n",
    "\n",
    "w1 = w1 - alpha*dw1\n",
    "w2 = w2 - alpha*dw2\n",
    "b = b - alpha*db\n",
    "```\n",
    "\n",
    "VERSION 2  \n",
    "pseudo | **python** | scala\n",
    "```python\n",
    "for i in range(ITER_NR):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigma(Z)\n",
    "    dZ = A - Y\n",
    "    dW = 1/M * X * dZ.T\n",
    "    db = 1/M * np.sum(dZ)\n",
    "    W = W - alpha * dW\n",
    "    b = b - alpha * db\n",
    "```\n",
    "\n",
    "## Get differentiation values\n",
    "\n",
    "$$\\frac{\\partial}{\\partial u_i}\\mathcal{L}(u_1,\\,...,u_i,\\,...,u_{3mn + 1})\\tag{2}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Differentiate $(C)$ in respect to **B**. Differentiate $(C)$ in respect to **A**. Differentiate $(C)$ in respect to $u_i$. Multiply the values (chain rule) and get the differentiation for the each of the current values $u_i$.\n",
    "\n",
    "## Update values in case the differentiation is negative\n",
    "\n",
    "$$u_i := u_i - \\alpha \\frac{\\partial}{\\partial u_i}\\mathcal{L}(u_1,\\,...,u_i,\\,...,u_{3mn + 1})\\tag{3}$$\n",
    "\n",
    "\n",
    "A single iteration of the gradient descent:\n",
    "\n",
    "pseudo | **python** | scala\n",
    "\n",
    "```python\n",
    "# forward\n",
    "Z = np.dot(W,X) + b\n",
    "A = sigma(Z)\n",
    "\n",
    "# backward\n",
    "dZ = A - Y\n",
    "dW = dW = ((1 / M) * X * dZ).sum(axis = 1).reshape(len(X), 1)\n",
    "db = (1 / M) * np.sum(dZ)  \n",
    "\n",
    "# update values\n",
    "W -= alpha * dW\n",
    "b -= alpha * db\n",
    "```\n",
    "\n",
    "<a id='rate_of_descent'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training / Minimizing: Gradient Descent](#gradient_descent)\n",
    "\n",
    "-----\n",
    "\n",
    "# Rate of descent\n",
    "\n",
    "The rate of descent determines $\\alpha$ how steep the gradient descent performs. If $\\alpha$ is too high, the cost function can oscilate and even diverge. If it is too small, too many iterations may be required to reach the minimum and the learning curve will ve very flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='computation_graph'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression / Training / Minimizing: Gradient Descent](#gradient_descent)\n",
    "\n",
    "-----\n",
    "# Computation Graph\n",
    "\n",
    "The neural network for binary classification consists of four nodes and $n\\,*\\,n\\,*\\,3\\,+\\,1$ inputs for $W, b$:\n",
    "\n",
    "$$W^TX+b\\tag{A}$$\n",
    "$$\\sigma(A)\\tag{B}$$\n",
    "$$\\mathcal{L}(W, b)\\tag{C}$$\n",
    "$$\\mathcal{C}(X, B)\\tag{D}$$\n",
    "\n",
    "\n",
    "<img src=\"img/binary_classification_neural_network_without_cost\" alt=\"Computation Graph\"/>\n",
    "\n",
    "$(D)$ is left out considering it does only take the simetric of the average of all loss.\n",
    "\n",
    "<a id='classification'></a>\n",
    "[/ Artificial Intelligence / Machine Learning / Deep Learning / Supervised Learning / Algorithms / Logistic Regression](#logistic_regression)\n",
    "\n",
    "-----\n",
    "\n",
    "## Classifying\n",
    "\n",
    "### Output formatting\n",
    "\n",
    "The classification is the mapping:\n",
    "\n",
    "$$LR : x^{(i)} \\in TS \\longrightarrow \\hat{y}^{(i)} \\in \\{0, 1\\} \\tag{1}$$\n",
    "\n",
    "The classifications of $X$ are assembled in the **vector** $Y$:\n",
    "\n",
    "$$ Y = (\\hat{y}^{(1)}\\,.\\,.\\,.\\,\\hat{y}^{(M)}) \\tag{4}$$\n",
    "\n",
    "Classifying (**python**):\n",
    "\n",
    "```python\n",
    "# upon optimal solution\n",
    "A = sigmoid(np.dot(w.T, X) + b)\n",
    "for i in range(A.shape[1]):\n",
    "    Y[0][i] = 0 if (A[0][i] < 0.5) else 1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
